
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 
2025-03-15 18:35:00.793399: Using torch.compile... 
2025-03-15 18:35:05.928910: do_dummy_2d_data_aug: False 
2025-03-15 18:35:05.929785: Using splits from existing split file: /mrhung_nguyen_minh_quang_108/workspace/train/nnUNet_preprocessed/Dataset015_lungTumor/splits_final.json 
2025-03-15 18:35:05.929994: The split file contains 5 splits. 
2025-03-15 18:35:05.930037: Desired fold for training: 0 
2025-03-15 18:35:05.930075: This split has 92 training and 23 validation cases. 

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [80, 192, 160], 'median_image_size_in_voxels': [251.0, 512.0, 512.0], 'spacing': [1.25, 0.78125, 0.78125], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.ResidualEncoderUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_blocks_per_stage': [1, 3, 4, 6, 6, 6], 'n_conv_per_stage_decoder': [1, 1, 1, 1, 1], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset015_lungTumor', 'plans_name': 'nnUNetResEncUNetMPlans', 'original_median_spacing_after_transp': [1.25, 0.78125, 0.78125], 'original_median_shape_after_transp': [251, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'nnUNetPlannerResEncM', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 2750.0, 'mean': -292.26348876953125, 'median': -205.0, 'min': -1270.0, 'percentile_00_5': -1024.0, 'percentile_99_5': 308.0, 'std': 352.5594787597656}}} 
 
2025-03-15 18:35:07.547178: Unable to plot network architecture: nnUNet_compile is enabled! 
2025-03-15 18:35:07.584013:  
2025-03-15 18:35:07.584276: Epoch 650 
2025-03-15 18:35:07.584491: Current learning rate: 0.00389 
2025-03-15 18:39:55.856111: train_loss -0.8444 
2025-03-15 18:39:55.856450: val_loss -0.6765 
2025-03-15 18:39:55.856540: Pseudo dice [0.8091] 
2025-03-15 18:39:55.856621: Epoch time: 288.28 s 
2025-03-15 18:39:55.856675: Yayy! New best EMA pseudo Dice: 0.7972 
2025-03-15 18:39:59.225741:  
2025-03-15 18:39:59.226196: Epoch 651 
2025-03-15 18:39:59.226352: Current learning rate: 0.00388 
2025-03-15 18:44:34.585591: train_loss -0.8325 
2025-03-15 18:44:34.585885: val_loss -0.7406 
2025-03-15 18:44:34.585974: Pseudo dice [0.8416] 
2025-03-15 18:44:34.586076: Epoch time: 275.36 s 
2025-03-15 18:44:34.586138: Yayy! New best EMA pseudo Dice: 0.8016 
2025-03-15 18:44:37.883224:  
2025-03-15 18:44:37.883372: Epoch 652 
2025-03-15 18:44:37.883489: Current learning rate: 0.00387 
2025-03-15 18:49:13.018832: train_loss -0.8338 
2025-03-15 18:49:13.019084: val_loss -0.7582 
2025-03-15 18:49:13.019159: Pseudo dice [0.8373] 
2025-03-15 18:49:13.019240: Epoch time: 275.14 s 
2025-03-15 18:49:13.019317: Yayy! New best EMA pseudo Dice: 0.8052 
2025-03-15 18:49:16.586983:  
2025-03-15 18:49:16.587210: Epoch 653 
2025-03-15 18:49:16.587385: Current learning rate: 0.00386 
2025-03-15 18:53:52.069751: train_loss -0.8139 
2025-03-15 18:53:52.070041: val_loss -0.6952 
2025-03-15 18:53:52.070125: Pseudo dice [0.7575] 
2025-03-15 18:53:52.070210: Epoch time: 275.49 s 
2025-03-15 18:53:54.000625:  
2025-03-15 18:53:54.000810: Epoch 654 
2025-03-15 18:53:54.000930: Current learning rate: 0.00385 
2025-03-15 18:58:29.389351: train_loss -0.8403 
2025-03-15 18:58:29.389626: val_loss -0.6901 
2025-03-15 18:58:29.389708: Pseudo dice [0.7993] 
2025-03-15 18:58:29.389890: Epoch time: 275.39 s 
2025-03-15 18:58:31.317189:  
2025-03-15 18:58:31.317371: Epoch 655 
2025-03-15 18:58:31.317534: Current learning rate: 0.00384 
2025-03-15 19:03:06.725295: train_loss -0.8359 
2025-03-15 19:03:06.725603: val_loss -0.7051 
2025-03-15 19:03:06.725707: Pseudo dice [0.7791] 
2025-03-15 19:03:06.725800: Epoch time: 275.41 s 
2025-03-15 19:03:08.650385:  
2025-03-15 19:03:08.650586: Epoch 656 
2025-03-15 19:03:08.650707: Current learning rate: 0.00383 
2025-03-15 19:07:43.701191: train_loss -0.841 
2025-03-15 19:07:43.701542: val_loss -0.7587 
2025-03-15 19:07:43.701645: Pseudo dice [0.8368] 
2025-03-15 19:07:43.701739: Epoch time: 275.05 s 
2025-03-15 19:07:45.628448:  
2025-03-15 19:07:45.628675: Epoch 657 
2025-03-15 19:07:45.628801: Current learning rate: 0.00382 
2025-03-15 19:12:20.237446: train_loss -0.8404 
2025-03-15 19:12:20.237789: val_loss -0.7714 
2025-03-15 19:12:20.237896: Pseudo dice [0.834] 
2025-03-15 19:12:20.237994: Epoch time: 274.61 s 
2025-03-15 19:12:20.238059: Yayy! New best EMA pseudo Dice: 0.8053 
2025-03-15 19:12:23.520142:  
2025-03-15 19:12:23.520410: Epoch 658 
2025-03-15 19:12:23.520536: Current learning rate: 0.00381 
2025-03-15 19:16:57.869113: train_loss -0.8306 
2025-03-15 19:16:57.869425: val_loss -0.7239 
2025-03-15 19:16:57.869516: Pseudo dice [0.8261] 
2025-03-15 19:16:57.869661: Epoch time: 274.35 s 
2025-03-15 19:16:57.869725: Yayy! New best EMA pseudo Dice: 0.8073 
2025-03-15 19:17:01.168756:  
2025-03-15 19:17:01.168947: Epoch 659 
2025-03-15 19:17:01.169058: Current learning rate: 0.0038 
2025-03-15 19:21:34.828063: train_loss -0.8419 
2025-03-15 19:21:34.828348: val_loss -0.7072 
2025-03-15 19:21:34.828434: Pseudo dice [0.8042] 
2025-03-15 19:21:34.828539: Epoch time: 273.66 s 
2025-03-15 19:21:36.759155:  
2025-03-15 19:21:36.759371: Epoch 660 
2025-03-15 19:21:36.759488: Current learning rate: 0.00379 
2025-03-15 19:26:10.807536: train_loss -0.8343 
2025-03-15 19:26:10.807866: val_loss -0.692 
2025-03-15 19:26:10.807956: Pseudo dice [0.7849] 
2025-03-15 19:26:10.808053: Epoch time: 274.05 s 
2025-03-15 19:26:12.745149:  
2025-03-15 19:26:12.745349: Epoch 661 
2025-03-15 19:26:12.745471: Current learning rate: 0.00378 
2025-03-15 19:30:47.021639: train_loss -0.8332 
2025-03-15 19:30:47.021936: val_loss -0.7232 
2025-03-15 19:30:47.022018: Pseudo dice [0.816] 
2025-03-15 19:30:47.022124: Epoch time: 274.28 s 
2025-03-15 19:30:48.968851:  
2025-03-15 19:30:48.969052: Epoch 662 
2025-03-15 19:30:48.969166: Current learning rate: 0.00377 
2025-03-15 19:35:23.703526: train_loss -0.7938 
2025-03-15 19:35:23.703856: val_loss -0.7048 
2025-03-15 19:35:23.703944: Pseudo dice [0.7481] 
2025-03-15 19:35:23.704098: Epoch time: 274.74 s 
2025-03-15 19:35:25.629170:  
2025-03-15 19:35:25.629347: Epoch 663 
2025-03-15 19:35:25.629462: Current learning rate: 0.00376 
2025-03-15 19:40:00.076600: train_loss -0.8479 
2025-03-15 19:40:00.076925: val_loss -0.7492 
2025-03-15 19:40:00.077015: Pseudo dice [0.8694] 
2025-03-15 19:40:00.077154: Epoch time: 274.45 s 
2025-03-15 19:40:02.009382:  
2025-03-15 19:40:02.009588: Epoch 664 
2025-03-15 19:40:02.009706: Current learning rate: 0.00375 
2025-03-15 19:44:36.154358: train_loss -0.8301 
2025-03-15 19:44:36.154646: val_loss -0.754 
2025-03-15 19:44:36.154781: Pseudo dice [0.8283] 
2025-03-15 19:44:36.154880: Epoch time: 274.15 s 
2025-03-15 19:44:36.154941: Yayy! New best EMA pseudo Dice: 0.8092 
2025-03-15 19:44:39.479389:  
2025-03-15 19:44:39.479573: Epoch 665 
2025-03-15 19:44:39.479744: Current learning rate: 0.00374 
2025-03-15 19:49:13.613348: train_loss -0.8408 
2025-03-15 19:49:13.613755: val_loss -0.7187 
2025-03-15 19:49:13.613869: Pseudo dice [0.8135] 
2025-03-15 19:49:13.613948: Epoch time: 274.14 s 
2025-03-15 19:49:13.614004: Yayy! New best EMA pseudo Dice: 0.8096 
2025-03-15 19:49:16.937533:  
2025-03-15 19:49:16.937733: Epoch 666 
2025-03-15 19:49:16.937903: Current learning rate: 0.00373 
2025-03-15 19:53:51.523354: train_loss -0.8358 
2025-03-15 19:53:51.523659: val_loss -0.7258 
2025-03-15 19:53:51.523757: Pseudo dice [0.8281] 
2025-03-15 19:53:51.523856: Epoch time: 274.59 s 
2025-03-15 19:53:51.523916: Yayy! New best EMA pseudo Dice: 0.8115 
2025-03-15 19:53:55.166513:  
2025-03-15 19:53:55.166679: Epoch 667 
2025-03-15 19:53:55.166858: Current learning rate: 0.00372 
2025-03-15 19:58:30.166531: train_loss -0.8259 
2025-03-15 19:58:30.166862: val_loss -0.7388 
2025-03-15 19:58:30.166944: Pseudo dice [0.8267] 
2025-03-15 19:58:30.167056: Epoch time: 275.0 s 
2025-03-15 19:58:30.167118: Yayy! New best EMA pseudo Dice: 0.813 
2025-03-15 19:58:33.503193:  
2025-03-15 19:58:33.503448: Epoch 668 
2025-03-15 19:58:33.503607: Current learning rate: 0.00371 
2025-03-15 20:03:08.010528: train_loss -0.8203 
2025-03-15 20:03:08.010821: val_loss -0.7725 
2025-03-15 20:03:08.010899: Pseudo dice [0.8602] 
2025-03-15 20:03:08.010981: Epoch time: 274.51 s 
2025-03-15 20:03:08.011050: Yayy! New best EMA pseudo Dice: 0.8177 
2025-03-15 20:03:11.332901:  
2025-03-15 20:03:11.333142: Epoch 669 
2025-03-15 20:03:11.333280: Current learning rate: 0.0037 
2025-03-15 20:07:45.748600: train_loss -0.8172 
2025-03-15 20:07:45.748896: val_loss -0.7688 
2025-03-15 20:07:45.748986: Pseudo dice [0.844] 
2025-03-15 20:07:45.749073: Epoch time: 274.42 s 
2025-03-15 20:07:45.749182: Yayy! New best EMA pseudo Dice: 0.8204 
2025-03-15 20:07:49.110502:  
2025-03-15 20:07:49.110743: Epoch 670 
2025-03-15 20:07:49.110873: Current learning rate: 0.00369 
2025-03-15 20:12:23.477251: train_loss -0.8335 
2025-03-15 20:12:23.477541: val_loss -0.762 
2025-03-15 20:12:23.477628: Pseudo dice [0.8563] 
2025-03-15 20:12:23.477762: Epoch time: 274.37 s 
2025-03-15 20:12:23.477821: Yayy! New best EMA pseudo Dice: 0.824 
2025-03-15 20:12:26.836849:  
2025-03-15 20:12:26.837061: Epoch 671 
2025-03-15 20:12:26.837179: Current learning rate: 0.00368 
2025-03-15 20:17:01.234644: train_loss -0.8255 
2025-03-15 20:17:01.234972: val_loss -0.7195 
2025-03-15 20:17:01.235071: Pseudo dice [0.8152] 
2025-03-15 20:17:01.235155: Epoch time: 274.4 s 
2025-03-15 20:17:03.179713:  
2025-03-15 20:17:03.179938: Epoch 672 
2025-03-15 20:17:03.180099: Current learning rate: 0.00367 
2025-03-15 20:21:37.608580: train_loss -0.8317 
2025-03-15 20:21:37.608913: val_loss -0.7369 
2025-03-15 20:21:37.608996: Pseudo dice [0.825] 
2025-03-15 20:21:37.609091: Epoch time: 274.43 s 
2025-03-15 20:21:39.815022:  
2025-03-15 20:21:39.815237: Epoch 673 
2025-03-15 20:21:39.815365: Current learning rate: 0.00366 
2025-03-15 20:26:14.375801: train_loss -0.8043 
2025-03-15 20:26:14.376161: val_loss -0.5079 
2025-03-15 20:26:14.376248: Pseudo dice [0.4887] 
2025-03-15 20:26:14.376352: Epoch time: 274.56 s 
2025-03-15 20:26:16.336010:  
2025-03-15 20:26:16.336200: Epoch 674 
2025-03-15 20:26:16.336315: Current learning rate: 0.00365 
2025-03-15 20:30:50.974921: train_loss -0.8016 
2025-03-15 20:30:50.975199: val_loss -0.533 
2025-03-15 20:30:50.975280: Pseudo dice [0.6209] 
2025-03-15 20:30:50.975365: Epoch time: 274.64 s 
2025-03-15 20:30:52.932121:  
2025-03-15 20:30:52.932342: Epoch 675 
2025-03-15 20:30:52.932468: Current learning rate: 0.00364 
2025-03-15 20:35:26.882844: train_loss -0.8358 
2025-03-15 20:35:26.883223: val_loss -0.6326 
2025-03-15 20:35:26.883303: Pseudo dice [0.6676] 
2025-03-15 20:35:26.883396: Epoch time: 273.95 s 
2025-03-15 20:35:28.841624:  
2025-03-15 20:35:28.841821: Epoch 676 
2025-03-15 20:35:28.841934: Current learning rate: 0.00363 
2025-03-15 20:40:03.177378: train_loss -0.8236 
2025-03-15 20:40:03.177718: val_loss -0.6813 
2025-03-15 20:40:03.177805: Pseudo dice [0.6972] 
2025-03-15 20:40:03.177899: Epoch time: 274.34 s 
2025-03-15 20:40:05.149929:  
2025-03-15 20:40:05.150170: Epoch 677 
2025-03-15 20:40:05.150290: Current learning rate: 0.00362 
2025-03-15 20:44:39.960487: train_loss -0.8367 
2025-03-15 20:44:39.960828: val_loss -0.6835 
2025-03-15 20:44:39.960903: Pseudo dice [0.739] 
2025-03-15 20:44:39.960995: Epoch time: 274.81 s 
2025-03-15 20:44:41.917935:  
2025-03-15 20:44:41.918162: Epoch 678 
2025-03-15 20:44:41.918305: Current learning rate: 0.00361 
2025-03-15 20:49:16.382763: train_loss -0.8427 
2025-03-15 20:49:16.383186: val_loss -0.6714 
2025-03-15 20:49:16.383290: Pseudo dice [0.7633] 
2025-03-15 20:49:16.383376: Epoch time: 274.47 s 
2025-03-15 20:49:18.368491:  
2025-03-15 20:49:18.368674: Epoch 679 
2025-03-15 20:49:18.368788: Current learning rate: 0.0036 
2025-03-15 20:53:52.922416: train_loss -0.8441 
2025-03-15 20:53:52.922791: val_loss -0.7572 
2025-03-15 20:53:52.922968: Pseudo dice [0.8686] 
2025-03-15 20:53:52.923079: Epoch time: 274.56 s 
2025-03-15 20:53:54.891978:  
2025-03-15 20:53:54.892159: Epoch 680 
2025-03-15 20:53:54.892273: Current learning rate: 0.00359 
2025-03-15 20:58:29.684365: train_loss -0.8401 
2025-03-15 20:58:29.684677: val_loss -0.6996 
2025-03-15 20:58:29.684759: Pseudo dice [0.7861] 
2025-03-15 20:58:29.684904: Epoch time: 274.8 s 
2025-03-15 20:58:31.920220:  
2025-03-15 20:58:31.920542: Epoch 681 
2025-03-15 20:58:31.920675: Current learning rate: 0.00358 
2025-03-15 21:03:06.450668: train_loss -0.8451 
2025-03-15 21:03:06.450965: val_loss -0.6958 
2025-03-15 21:03:06.451051: Pseudo dice [0.8076] 
2025-03-15 21:03:06.451139: Epoch time: 274.53 s 
2025-03-15 21:03:08.407923:  
2025-03-15 21:03:08.408100: Epoch 682 
2025-03-15 21:03:08.408216: Current learning rate: 0.00357 
2025-03-15 21:07:43.223916: train_loss -0.8406 
2025-03-15 21:07:43.224204: val_loss -0.7053 
2025-03-15 21:07:43.224281: Pseudo dice [0.776] 
2025-03-15 21:07:43.224364: Epoch time: 274.82 s 
2025-03-15 21:07:45.185749:  
2025-03-15 21:07:45.185949: Epoch 683 
2025-03-15 21:07:45.186070: Current learning rate: 0.00356 
2025-03-15 21:12:19.980694: train_loss -0.8479 
2025-03-15 21:12:19.981060: val_loss -0.7425 
2025-03-15 21:12:19.981142: Pseudo dice [0.8298] 
2025-03-15 21:12:19.981237: Epoch time: 274.8 s 
2025-03-15 21:12:21.952582:  
2025-03-15 21:12:21.952765: Epoch 684 
2025-03-15 21:12:21.952888: Current learning rate: 0.00355 
2025-03-15 21:16:56.042337: train_loss -0.8461 
2025-03-15 21:16:56.042666: val_loss -0.7315 
2025-03-15 21:16:56.042752: Pseudo dice [0.8368] 
2025-03-15 21:16:56.042847: Epoch time: 274.09 s 
2025-03-15 21:16:57.994957:  
2025-03-15 21:16:57.995121: Epoch 685 
2025-03-15 21:16:57.995236: Current learning rate: 0.00354 
2025-03-15 21:21:31.670074: train_loss -0.8568 
2025-03-15 21:21:31.670363: val_loss -0.7497 
2025-03-15 21:21:31.670497: Pseudo dice [0.8233] 
2025-03-15 21:21:31.670658: Epoch time: 273.68 s 
2025-03-15 21:21:33.634469:  
2025-03-15 21:21:33.634777: Epoch 686 
2025-03-15 21:21:33.634913: Current learning rate: 0.00353 
2025-03-15 21:26:07.418334: train_loss -0.8533 
2025-03-15 21:26:07.418626: val_loss -0.7294 
2025-03-15 21:26:07.418768: Pseudo dice [0.8193] 
2025-03-15 21:26:07.418864: Epoch time: 273.79 s 
2025-03-15 21:26:09.375427:  
2025-03-15 21:26:09.375666: Epoch 687 
2025-03-15 21:26:09.375788: Current learning rate: 0.00352 
2025-03-15 21:30:42.935734: train_loss -0.8609 
2025-03-15 21:30:42.936062: val_loss -0.7071 
2025-03-15 21:30:42.936140: Pseudo dice [0.7515] 
2025-03-15 21:30:42.936231: Epoch time: 273.56 s 
2025-03-15 21:30:44.889351:  
2025-03-15 21:30:44.889609: Epoch 688 
2025-03-15 21:30:44.889724: Current learning rate: 0.00351 
2025-03-15 21:35:18.465564: train_loss -0.847 
2025-03-15 21:35:18.465844: val_loss -0.7016 
2025-03-15 21:35:18.465918: Pseudo dice [0.8132] 
2025-03-15 21:35:18.466001: Epoch time: 273.58 s 
2025-03-15 21:35:20.729766:  
2025-03-15 21:35:20.729992: Epoch 689 
2025-03-15 21:35:20.730124: Current learning rate: 0.0035 
2025-03-15 21:39:54.301455: train_loss -0.8544 
2025-03-15 21:39:54.301758: val_loss -0.7513 
2025-03-15 21:39:54.301853: Pseudo dice [0.8549] 
2025-03-15 21:39:54.301937: Epoch time: 273.58 s 
2025-03-15 21:39:56.265724:  
2025-03-15 21:39:56.265919: Epoch 690 
2025-03-15 21:39:56.266057: Current learning rate: 0.00349 
2025-03-15 21:44:30.185204: train_loss -0.8519 
2025-03-15 21:44:30.185487: val_loss -0.7573 
2025-03-15 21:44:30.185569: Pseudo dice [0.8347] 
2025-03-15 21:44:30.185677: Epoch time: 273.92 s 
2025-03-15 21:44:32.124108:  
2025-03-15 21:44:32.124281: Epoch 691 
2025-03-15 21:44:32.124401: Current learning rate: 0.00348 
2025-03-15 21:49:06.208344: train_loss -0.8428 
2025-03-15 21:49:06.208634: val_loss -0.6959 
2025-03-15 21:49:06.208716: Pseudo dice [0.8277] 
2025-03-15 21:49:06.208804: Epoch time: 274.09 s 
2025-03-15 21:49:08.158833:  
2025-03-15 21:49:08.159012: Epoch 692 
2025-03-15 21:49:08.159126: Current learning rate: 0.00346 
2025-03-15 21:53:43.019135: train_loss -0.8352 
2025-03-15 21:53:43.019454: val_loss -0.7282 
2025-03-15 21:53:43.019560: Pseudo dice [0.796] 
2025-03-15 21:53:43.019663: Epoch time: 274.86 s 
2025-03-15 21:53:44.966236:  
2025-03-15 21:53:44.966419: Epoch 693 
2025-03-15 21:53:44.966553: Current learning rate: 0.00345 
2025-03-15 21:58:19.564234: train_loss -0.8492 
2025-03-15 21:58:19.564536: val_loss -0.7233 
2025-03-15 21:58:19.564697: Pseudo dice [0.797] 
2025-03-15 21:58:19.564809: Epoch time: 274.6 s 
2025-03-15 21:58:21.511349:  
2025-03-15 21:58:21.511529: Epoch 694 
2025-03-15 21:58:21.511647: Current learning rate: 0.00344 
2025-03-15 22:02:56.072910: train_loss -0.8564 
2025-03-15 22:02:56.073368: val_loss -0.7198 
2025-03-15 22:02:56.073460: Pseudo dice [0.7993] 
2025-03-15 22:02:56.073548: Epoch time: 274.57 s 
2025-03-15 22:02:58.027422:  
2025-03-15 22:02:58.027577: Epoch 695 
2025-03-15 22:02:58.027694: Current learning rate: 0.00343 
2025-03-15 22:07:32.790912: train_loss -0.851 
2025-03-15 22:07:32.791198: val_loss -0.7528 
2025-03-15 22:07:32.791378: Pseudo dice [0.8369] 
2025-03-15 22:07:32.791507: Epoch time: 274.77 s 
2025-03-15 22:07:34.762028:  
2025-03-15 22:07:34.762199: Epoch 696 
2025-03-15 22:07:34.762339: Current learning rate: 0.00342 
2025-03-15 22:12:09.330288: train_loss -0.8534 
2025-03-15 22:12:09.330733: val_loss -0.718 
2025-03-15 22:12:09.330850: Pseudo dice [0.7831] 
2025-03-15 22:12:09.330927: Epoch time: 274.57 s 
2025-03-15 22:12:11.287692:  
2025-03-15 22:12:11.287914: Epoch 697 
2025-03-15 22:12:11.288030: Current learning rate: 0.00341 
2025-03-15 22:16:45.898295: train_loss -0.8518 
2025-03-15 22:16:45.898636: val_loss -0.6384 
2025-03-15 22:16:45.898741: Pseudo dice [0.6668] 
2025-03-15 22:16:45.898849: Epoch time: 274.61 s 
2025-03-15 22:16:47.898216:  
2025-03-15 22:16:47.898370: Epoch 698 
2025-03-15 22:16:47.898504: Current learning rate: 0.0034 
2025-03-15 22:21:22.454886: train_loss -0.8556 
2025-03-15 22:21:22.455179: val_loss -0.6755 
2025-03-15 22:21:22.455280: Pseudo dice [0.6991] 
2025-03-15 22:21:22.455418: Epoch time: 274.56 s 
2025-03-15 22:21:24.408367:  
2025-03-15 22:21:24.408628: Epoch 699 
2025-03-15 22:21:24.408778: Current learning rate: 0.00339 
2025-03-15 22:25:59.137203: train_loss -0.8425 
2025-03-15 22:25:59.137517: val_loss -0.6929 
2025-03-15 22:25:59.137598: Pseudo dice [0.7375] 
2025-03-15 22:25:59.137689: Epoch time: 274.73 s 
2025-03-15 22:26:02.531842:  
2025-03-15 22:26:02.532043: Epoch 700 
2025-03-15 22:26:02.532162: Current learning rate: 0.00338 
2025-03-15 22:30:37.471828: train_loss -0.8533 
2025-03-15 22:30:37.472135: val_loss -0.7773 
2025-03-15 22:30:37.472237: Pseudo dice [0.8505] 
2025-03-15 22:30:37.472342: Epoch time: 274.94 s 
2025-03-15 22:30:39.442331:  
2025-03-15 22:30:39.442586: Epoch 701 
2025-03-15 22:30:39.442709: Current learning rate: 0.00337 
2025-03-15 22:35:14.112102: train_loss -0.8583 
2025-03-15 22:35:14.112432: val_loss -0.7353 
2025-03-15 22:35:14.112517: Pseudo dice [0.777] 
2025-03-15 22:35:14.112615: Epoch time: 274.67 s 
2025-03-15 22:35:16.067133:  
2025-03-15 22:35:16.067412: Epoch 702 
2025-03-15 22:35:16.067567: Current learning rate: 0.00336 
2025-03-15 22:39:50.651807: train_loss -0.8534 
2025-03-15 22:39:50.652144: val_loss -0.7504 
2025-03-15 22:39:50.652230: Pseudo dice [0.813] 
2025-03-15 22:39:50.652326: Epoch time: 274.59 s 
2025-03-15 22:39:52.598835:  
2025-03-15 22:39:52.599117: Epoch 703 
2025-03-15 22:39:52.599247: Current learning rate: 0.00335 
2025-03-15 22:44:26.866304: train_loss -0.8196 
2025-03-15 22:44:26.866606: val_loss -0.5587 
2025-03-15 22:44:26.866684: Pseudo dice [0.5431] 
2025-03-15 22:44:26.866777: Epoch time: 274.27 s 
2025-03-15 22:44:29.094699:  
2025-03-15 22:44:29.094934: Epoch 704 
2025-03-15 22:44:29.095051: Current learning rate: 0.00334 
2025-03-15 22:49:03.754336: train_loss -0.8279 
2025-03-15 22:49:03.754635: val_loss -0.6113 
2025-03-15 22:49:03.754712: Pseudo dice [0.6958] 
2025-03-15 22:49:03.754803: Epoch time: 274.66 s 
2025-03-15 22:49:05.730266:  
2025-03-15 22:49:05.730456: Epoch 705 
2025-03-15 22:49:05.730573: Current learning rate: 0.00333 
2025-03-15 22:53:40.464718: train_loss -0.8287 
2025-03-15 22:53:40.465067: val_loss -0.7385 
2025-03-15 22:53:40.465185: Pseudo dice [0.8476] 
2025-03-15 22:53:40.465278: Epoch time: 274.74 s 
2025-03-15 22:53:42.433878:  
2025-03-15 22:53:42.434133: Epoch 706 
2025-03-15 22:53:42.434270: Current learning rate: 0.00332 
2025-03-15 22:58:16.760492: train_loss -0.8181 
2025-03-15 22:58:16.760811: val_loss -0.6177 
2025-03-15 22:58:16.760896: Pseudo dice [0.7193] 
2025-03-15 22:58:16.760997: Epoch time: 274.33 s 
2025-03-15 22:58:18.739931:  
2025-03-15 22:58:18.740117: Epoch 707 
2025-03-15 22:58:18.740235: Current learning rate: 0.00331 
2025-03-15 23:02:53.474579: train_loss -0.7957 
2025-03-15 23:02:53.474872: val_loss -0.7467 
2025-03-15 23:02:53.474954: Pseudo dice [0.814] 
2025-03-15 23:02:53.475111: Epoch time: 274.74 s 
2025-03-15 23:02:55.441833:  
2025-03-15 23:02:55.442031: Epoch 708 
2025-03-15 23:02:55.442153: Current learning rate: 0.0033 
2025-03-15 23:07:29.864884: train_loss -0.8249 
2025-03-15 23:07:29.865166: val_loss -0.6731 
2025-03-15 23:07:29.865243: Pseudo dice [0.7543] 
2025-03-15 23:07:29.865331: Epoch time: 274.43 s 
2025-03-15 23:07:31.815882:  
2025-03-15 23:07:31.816124: Epoch 709 
2025-03-15 23:07:31.816240: Current learning rate: 0.00329 
2025-03-15 23:12:06.291642: train_loss -0.8237 
2025-03-15 23:12:06.291981: val_loss -0.694 
2025-03-15 23:12:06.292058: Pseudo dice [0.7701] 
2025-03-15 23:12:06.292153: Epoch time: 274.48 s 
2025-03-15 23:12:08.258021:  
2025-03-15 23:12:08.258265: Epoch 710 
2025-03-15 23:12:08.258404: Current learning rate: 0.00328 
2025-03-15 23:16:42.388860: train_loss -0.8313 
2025-03-15 23:16:42.389160: val_loss -0.5996 
2025-03-15 23:16:42.389243: Pseudo dice [0.6273] 
2025-03-15 23:16:42.389344: Epoch time: 274.13 s 
2025-03-15 23:16:44.371129:  
2025-03-15 23:16:44.371315: Epoch 711 
2025-03-15 23:16:44.371428: Current learning rate: 0.00327 
2025-03-15 23:21:18.993689: train_loss -0.8486 
2025-03-15 23:21:18.994027: val_loss -0.7352 
2025-03-15 23:21:18.994108: Pseudo dice [0.8107] 
2025-03-15 23:21:18.994189: Epoch time: 274.63 s 
2025-03-15 23:21:21.290686:  
2025-03-15 23:21:21.290893: Epoch 712 
2025-03-15 23:21:21.291009: Current learning rate: 0.00326 
2025-03-15 23:25:55.557015: train_loss -0.8544 
2025-03-15 23:25:55.557303: val_loss -0.6862 
2025-03-15 23:25:55.557384: Pseudo dice [0.7936] 
2025-03-15 23:25:55.557467: Epoch time: 274.27 s 
2025-03-15 23:25:57.535354:  
2025-03-15 23:25:57.535672: Epoch 713 
2025-03-15 23:25:57.535814: Current learning rate: 0.00325 
2025-03-15 23:30:31.493405: train_loss -0.8466 
2025-03-15 23:30:31.493681: val_loss -0.7104 
2025-03-15 23:30:31.493760: Pseudo dice [0.7818] 
2025-03-15 23:30:31.493845: Epoch time: 273.96 s 
2025-03-15 23:30:33.460125:  
2025-03-15 23:30:33.460339: Epoch 714 
2025-03-15 23:30:33.460452: Current learning rate: 0.00324 
2025-03-15 23:35:07.229069: train_loss -0.8504 
2025-03-15 23:35:07.229412: val_loss -0.7226 
2025-03-15 23:35:07.229513: Pseudo dice [0.8131] 
2025-03-15 23:35:07.229609: Epoch time: 273.77 s 
2025-03-15 23:35:09.199588:  
2025-03-15 23:35:09.199816: Epoch 715 
2025-03-15 23:35:09.199942: Current learning rate: 0.00323 
2025-03-15 23:39:43.365416: train_loss -0.8567 
2025-03-15 23:39:43.365726: val_loss -0.7369 
2025-03-15 23:39:43.365837: Pseudo dice [0.832] 
2025-03-15 23:39:43.365938: Epoch time: 274.17 s 
2025-03-15 23:39:45.336503:  
2025-03-15 23:39:45.336695: Epoch 716 
2025-03-15 23:39:45.336824: Current learning rate: 0.00322 
2025-03-15 23:44:19.581957: train_loss -0.851 
2025-03-15 23:44:19.582331: val_loss -0.693 
2025-03-15 23:44:19.582411: Pseudo dice [0.758] 
2025-03-15 23:44:19.582502: Epoch time: 274.25 s 
2025-03-15 23:44:21.577721:  
2025-03-15 23:44:21.577959: Epoch 717 
2025-03-15 23:44:21.578076: Current learning rate: 0.00321 
2025-03-15 23:48:55.538960: train_loss -0.8538 
2025-03-15 23:48:55.539339: val_loss -0.5562 
2025-03-15 23:48:55.539441: Pseudo dice [0.6452] 
2025-03-15 23:48:55.539536: Epoch time: 273.97 s 
2025-03-15 23:48:57.521674:  
2025-03-15 23:48:57.521900: Epoch 718 
2025-03-15 23:48:57.522027: Current learning rate: 0.0032 
2025-03-15 23:53:31.861091: train_loss -0.8256 
2025-03-15 23:53:31.861475: val_loss -0.5629 
2025-03-15 23:53:31.861558: Pseudo dice [0.6759] 
2025-03-15 23:53:31.861655: Epoch time: 274.34 s 
2025-03-15 23:53:34.146227:  
2025-03-15 23:53:34.146476: Epoch 719 
2025-03-15 23:53:34.146599: Current learning rate: 0.00319 
2025-03-15 23:58:08.623498: train_loss -0.8406 
2025-03-15 23:58:08.623826: val_loss -0.6428 
2025-03-15 23:58:08.623922: Pseudo dice [0.6615] 
2025-03-15 23:58:08.624026: Epoch time: 274.48 s 
2025-03-15 23:58:10.597544:  
2025-03-15 23:58:10.597896: Epoch 720 
2025-03-15 23:58:10.598052: Current learning rate: 0.00318 
2025-03-16 00:02:45.291571: train_loss -0.8312 
2025-03-16 00:02:45.291936: val_loss -0.7039 
2025-03-16 00:02:45.292022: Pseudo dice [0.8541] 
2025-03-16 00:02:45.292121: Epoch time: 274.7 s 
2025-03-16 00:02:47.265457:  
2025-03-16 00:02:47.265709: Epoch 721 
2025-03-16 00:02:47.265873: Current learning rate: 0.00317 
2025-03-16 00:07:21.678122: train_loss -0.8468 
2025-03-16 00:07:21.678421: val_loss -0.6576 
2025-03-16 00:07:21.678514: Pseudo dice [0.8046] 
2025-03-16 00:07:21.678605: Epoch time: 274.42 s 
2025-03-16 00:07:23.644939:  
2025-03-16 00:07:23.645205: Epoch 722 
2025-03-16 00:07:23.645341: Current learning rate: 0.00316 
2025-03-16 00:11:57.642601: train_loss -0.8509 
2025-03-16 00:11:57.642940: val_loss -0.7812 
2025-03-16 00:11:57.643104: Pseudo dice [0.8631] 
2025-03-16 00:11:57.643184: Epoch time: 274.0 s 
2025-03-16 00:11:59.668940:  
2025-03-16 00:11:59.669138: Epoch 723 
2025-03-16 00:11:59.669257: Current learning rate: 0.00315 
2025-03-16 00:16:34.158243: train_loss -0.8496 
2025-03-16 00:16:34.158559: val_loss -0.716 
2025-03-16 00:16:34.158641: Pseudo dice [0.8363] 
2025-03-16 00:16:34.158734: Epoch time: 274.49 s 
2025-03-16 00:16:36.166930:  
2025-03-16 00:16:36.167126: Epoch 724 
2025-03-16 00:16:36.167239: Current learning rate: 0.00314 
2025-03-16 00:21:10.488176: train_loss -0.8362 
2025-03-16 00:21:10.488593: val_loss -0.7112 
2025-03-16 00:21:10.488690: Pseudo dice [0.8479] 
2025-03-16 00:21:10.488772: Epoch time: 274.33 s 
2025-03-16 00:21:12.455354:  
2025-03-16 00:21:12.455523: Epoch 725 
2025-03-16 00:21:12.455646: Current learning rate: 0.00313 
2025-03-16 00:25:46.953942: train_loss -0.8366 
2025-03-16 00:25:46.954260: val_loss -0.6773 
2025-03-16 00:25:46.954415: Pseudo dice [0.8129] 
2025-03-16 00:25:46.954512: Epoch time: 274.5 s 
2025-03-16 00:25:48.930293:  
2025-03-16 00:25:48.930547: Epoch 726 
2025-03-16 00:25:48.930688: Current learning rate: 0.00312 
2025-03-16 00:30:22.930752: train_loss -0.8602 
2025-03-16 00:30:22.931103: val_loss -0.7163 
2025-03-16 00:30:22.931185: Pseudo dice [0.7966] 
2025-03-16 00:30:22.931282: Epoch time: 274.0 s 
2025-03-16 00:30:24.909031:  
2025-03-16 00:30:24.909245: Epoch 727 
2025-03-16 00:30:24.909411: Current learning rate: 0.00311 
2025-03-16 00:34:59.430075: train_loss -0.8495 
2025-03-16 00:34:59.430343: val_loss -0.8003 
2025-03-16 00:34:59.430418: Pseudo dice [0.8767] 
2025-03-16 00:34:59.430496: Epoch time: 274.52 s 
2025-03-16 00:35:01.399817:  
2025-03-16 00:35:01.399994: Epoch 728 
2025-03-16 00:35:01.400107: Current learning rate: 0.0031 
2025-03-16 00:39:35.227693: train_loss -0.8557 
2025-03-16 00:39:35.228059: val_loss -0.7425 
2025-03-16 00:39:35.228147: Pseudo dice [0.7915] 
2025-03-16 00:39:35.228256: Epoch time: 273.83 s 
2025-03-16 00:39:37.201931:  
2025-03-16 00:39:37.202222: Epoch 729 
2025-03-16 00:39:37.202354: Current learning rate: 0.00309 
2025-03-16 00:44:11.799475: train_loss -0.8422 
2025-03-16 00:44:11.799776: val_loss -0.7422 
2025-03-16 00:44:11.799868: Pseudo dice [0.848] 
2025-03-16 00:44:11.799972: Epoch time: 274.6 s 
2025-03-16 00:44:13.787897:  
2025-03-16 00:44:13.788046: Epoch 730 
2025-03-16 00:44:13.788184: Current learning rate: 0.00308 
2025-03-16 00:48:48.516883: train_loss -0.8473 
2025-03-16 00:48:48.517191: val_loss -0.7055 
2025-03-16 00:48:48.517275: Pseudo dice [0.8409] 
2025-03-16 00:48:48.517373: Epoch time: 274.73 s 
2025-03-16 00:48:50.473023:  
2025-03-16 00:48:50.473223: Epoch 731 
2025-03-16 00:48:50.473343: Current learning rate: 0.00307 
2025-03-16 00:53:25.156828: train_loss -0.8563 
2025-03-16 00:53:25.157167: val_loss -0.7085 
2025-03-16 00:53:25.157252: Pseudo dice [0.8234] 
2025-03-16 00:53:25.157346: Epoch time: 274.69 s 
2025-03-16 00:53:27.120378:  
2025-03-16 00:53:27.120687: Epoch 732 
2025-03-16 00:53:27.120806: Current learning rate: 0.00306 
2025-03-16 00:58:02.010988: train_loss -0.856 
2025-03-16 00:58:02.011344: val_loss -0.7539 
2025-03-16 00:58:02.011428: Pseudo dice [0.8492] 
2025-03-16 00:58:02.011530: Epoch time: 274.89 s 
2025-03-16 00:58:03.981802:  
2025-03-16 00:58:03.982046: Epoch 733 
2025-03-16 00:58:03.982179: Current learning rate: 0.00305 
2025-03-16 01:02:38.750968: train_loss -0.8608 
2025-03-16 01:02:38.751306: val_loss -0.6816 
2025-03-16 01:02:38.751393: Pseudo dice [0.8561] 
2025-03-16 01:02:38.751486: Epoch time: 274.77 s 
2025-03-16 01:02:40.703855:  
2025-03-16 01:02:40.704023: Epoch 734 
2025-03-16 01:02:40.704138: Current learning rate: 0.00304 
2025-03-16 01:07:15.268280: train_loss -0.8487 
2025-03-16 01:07:15.268585: val_loss -0.7255 
2025-03-16 01:07:15.268665: Pseudo dice [0.837] 
2025-03-16 01:07:15.268754: Epoch time: 274.57 s 
2025-03-16 01:07:17.518229:  
2025-03-16 01:07:17.518448: Epoch 735 
2025-03-16 01:07:17.518579: Current learning rate: 0.00303 
2025-03-16 01:11:52.310718: train_loss -0.8521 
2025-03-16 01:11:52.311090: val_loss -0.722 
2025-03-16 01:11:52.311177: Pseudo dice [0.8117] 
2025-03-16 01:11:52.311268: Epoch time: 274.8 s 
2025-03-16 01:11:54.278838:  
2025-03-16 01:11:54.279038: Epoch 736 
2025-03-16 01:11:54.279173: Current learning rate: 0.00302 
2025-03-16 01:16:28.867091: train_loss -0.8558 
2025-03-16 01:16:28.867422: val_loss -0.7565 
2025-03-16 01:16:28.867572: Pseudo dice [0.8424] 
2025-03-16 01:16:28.867683: Epoch time: 274.59 s 
2025-03-16 01:16:30.824935:  
2025-03-16 01:16:30.825194: Epoch 737 
2025-03-16 01:16:30.825310: Current learning rate: 0.00301 
2025-03-16 01:21:05.677848: train_loss -0.8568 
2025-03-16 01:21:05.678144: val_loss -0.6523 
2025-03-16 01:21:05.678228: Pseudo dice [0.841] 
2025-03-16 01:21:05.678408: Epoch time: 274.86 s 
2025-03-16 01:21:07.674240:  
2025-03-16 01:21:07.674418: Epoch 738 
2025-03-16 01:21:07.674535: Current learning rate: 0.003 
2025-03-16 01:25:42.814331: train_loss -0.8585 
2025-03-16 01:25:42.814623: val_loss -0.697 
2025-03-16 01:25:42.814701: Pseudo dice [0.8208] 
2025-03-16 01:25:42.814798: Epoch time: 275.14 s 
2025-03-16 01:25:44.777168:  
2025-03-16 01:25:44.777342: Epoch 739 
2025-03-16 01:25:44.777455: Current learning rate: 0.00299 
2025-03-16 01:30:19.866057: train_loss -0.8505 
2025-03-16 01:30:19.866375: val_loss -0.6899 
2025-03-16 01:30:19.866524: Pseudo dice [0.8575] 
2025-03-16 01:30:19.866623: Epoch time: 275.09 s 
2025-03-16 01:30:19.866680: Yayy! New best EMA pseudo Dice: 0.8252 
2025-03-16 01:30:23.218830:  
2025-03-16 01:30:23.219058: Epoch 740 
2025-03-16 01:30:23.219206: Current learning rate: 0.00297 
2025-03-16 01:34:58.400013: train_loss -0.832 
2025-03-16 01:34:58.400259: val_loss -0.6812 
2025-03-16 01:34:58.400341: Pseudo dice [0.869] 
2025-03-16 01:34:58.400437: Epoch time: 275.19 s 
2025-03-16 01:34:58.400496: Yayy! New best EMA pseudo Dice: 0.8296 
2025-03-16 01:35:01.792461:  
2025-03-16 01:35:01.792600: Epoch 741 
2025-03-16 01:35:01.792713: Current learning rate: 0.00296 
2025-03-16 01:39:36.523413: train_loss -0.8478 
2025-03-16 01:39:36.523713: val_loss -0.6491 
2025-03-16 01:39:36.523811: Pseudo dice [0.7989] 
2025-03-16 01:39:36.523904: Epoch time: 274.73 s 
2025-03-16 01:39:38.793328:  
2025-03-16 01:39:38.793500: Epoch 742 
2025-03-16 01:39:38.793616: Current learning rate: 0.00295 
2025-03-16 01:44:13.305240: train_loss -0.8541 
2025-03-16 01:44:13.305647: val_loss -0.6167 
2025-03-16 01:44:13.305742: Pseudo dice [0.8001] 
2025-03-16 01:44:13.305820: Epoch time: 274.52 s 
2025-03-16 01:44:15.279138:  
2025-03-16 01:44:15.279347: Epoch 743 
2025-03-16 01:44:15.279473: Current learning rate: 0.00294 
2025-03-16 01:48:49.483564: train_loss -0.8548 
2025-03-16 01:48:49.483894: val_loss -0.4127 
2025-03-16 01:48:49.483978: Pseudo dice [0.3966] 
2025-03-16 01:48:49.484177: Epoch time: 274.21 s 
2025-03-16 01:48:51.448346:  
2025-03-16 01:48:51.448568: Epoch 744 
2025-03-16 01:48:51.448694: Current learning rate: 0.00293 
2025-03-16 01:53:26.019197: train_loss -0.8308 
2025-03-16 01:53:26.019493: val_loss -0.6718 
2025-03-16 01:53:26.019578: Pseudo dice [0.7586] 
2025-03-16 01:53:26.019664: Epoch time: 274.57 s 
2025-03-16 01:53:27.979612:  
2025-03-16 01:53:27.979814: Epoch 745 
2025-03-16 01:53:27.979933: Current learning rate: 0.00292 
2025-03-16 01:58:02.740783: train_loss -0.8297 
2025-03-16 01:58:02.741214: val_loss -0.5808 
2025-03-16 01:58:02.741378: Pseudo dice [0.5545] 
2025-03-16 01:58:02.741463: Epoch time: 274.77 s 
2025-03-16 01:58:04.720047:  
2025-03-16 01:58:04.720216: Epoch 746 
2025-03-16 01:58:04.720329: Current learning rate: 0.00291 
2025-03-16 02:02:39.104186: train_loss -0.8468 
2025-03-16 02:02:39.104479: val_loss -0.6937 
2025-03-16 02:02:39.104578: Pseudo dice [0.782] 
2025-03-16 02:02:39.104667: Epoch time: 274.39 s 
2025-03-16 02:02:41.069308:  
2025-03-16 02:02:41.069493: Epoch 747 
2025-03-16 02:02:41.069627: Current learning rate: 0.0029 
2025-03-16 02:07:15.360050: train_loss -0.861 
2025-03-16 02:07:15.360351: val_loss -0.6815 
2025-03-16 02:07:15.360431: Pseudo dice [0.7861] 
2025-03-16 02:07:15.360509: Epoch time: 274.29 s 
2025-03-16 02:07:17.324446:  
2025-03-16 02:07:17.324693: Epoch 748 
2025-03-16 02:07:17.324808: Current learning rate: 0.00289 
2025-03-16 02:11:51.504774: train_loss -0.8488 
2025-03-16 02:11:51.505131: val_loss -0.7112 
2025-03-16 02:11:51.505214: Pseudo dice [0.789] 
2025-03-16 02:11:51.505321: Epoch time: 274.18 s 
2025-03-16 02:11:53.473478:  
2025-03-16 02:11:53.473681: Epoch 749 
2025-03-16 02:11:53.473796: Current learning rate: 0.00288 
2025-03-16 02:16:27.792484: train_loss -0.8519 
2025-03-16 02:16:27.792771: val_loss -0.4741 
2025-03-16 02:16:27.792852: Pseudo dice [0.5979] 
2025-03-16 02:16:27.792938: Epoch time: 274.32 s 
2025-03-16 02:16:31.468378:  
2025-03-16 02:16:31.468598: Epoch 750 
2025-03-16 02:16:31.468717: Current learning rate: 0.00287 
2025-03-16 02:21:05.796251: train_loss -0.8454 
2025-03-16 02:21:05.796547: val_loss -0.7127 
2025-03-16 02:21:05.796719: Pseudo dice [0.8095] 
2025-03-16 02:21:05.796830: Epoch time: 274.33 s 
2025-03-16 02:21:07.761099:  
2025-03-16 02:21:07.761281: Epoch 751 
2025-03-16 02:21:07.761397: Current learning rate: 0.00286 
2025-03-16 02:25:42.317067: train_loss -0.8465 
2025-03-16 02:25:42.317371: val_loss -0.7617 
2025-03-16 02:25:42.317447: Pseudo dice [0.8523] 
2025-03-16 02:25:42.317531: Epoch time: 274.56 s 
2025-03-16 02:25:44.290652:  
2025-03-16 02:25:44.290951: Epoch 752 
2025-03-16 02:25:44.291065: Current learning rate: 0.00285 
2025-03-16 02:30:19.039528: train_loss -0.8576 
2025-03-16 02:30:19.039872: val_loss -0.6907 
2025-03-16 02:30:19.039966: Pseudo dice [0.7703] 
2025-03-16 02:30:19.040070: Epoch time: 274.75 s 
2025-03-16 02:30:21.034453:  
2025-03-16 02:30:21.034732: Epoch 753 
2025-03-16 02:30:21.034851: Current learning rate: 0.00284 
2025-03-16 02:34:56.154702: train_loss -0.8414 
2025-03-16 02:34:56.155008: val_loss -0.7135 
2025-03-16 02:34:56.155109: Pseudo dice [0.8037] 
2025-03-16 02:34:56.155213: Epoch time: 275.12 s 
2025-03-16 02:34:58.132300:  
2025-03-16 02:34:58.132466: Epoch 754 
2025-03-16 02:34:58.132591: Current learning rate: 0.00283 
2025-03-16 02:39:33.155817: train_loss -0.8429 
2025-03-16 02:39:33.156167: val_loss -0.7197 
2025-03-16 02:39:33.156251: Pseudo dice [0.8522] 
2025-03-16 02:39:33.156343: Epoch time: 275.03 s 
2025-03-16 02:39:35.144325:  
2025-03-16 02:39:35.144510: Epoch 755 
2025-03-16 02:39:35.144639: Current learning rate: 0.00282 
2025-03-16 02:44:09.882897: train_loss -0.8537 
2025-03-16 02:44:09.883250: val_loss -0.5733 
2025-03-16 02:44:09.883373: Pseudo dice [0.6993] 
2025-03-16 02:44:09.883477: Epoch time: 274.74 s 
2025-03-16 02:44:11.853695:  
2025-03-16 02:44:11.853912: Epoch 756 
2025-03-16 02:44:11.854058: Current learning rate: 0.00281 
2025-03-16 02:48:46.785855: train_loss -0.8476 
2025-03-16 02:48:46.786158: val_loss -0.711 
2025-03-16 02:48:46.786241: Pseudo dice [0.8378] 
2025-03-16 02:48:46.786329: Epoch time: 274.94 s 
2025-03-16 02:48:48.772604:  
2025-03-16 02:48:48.772790: Epoch 757 
2025-03-16 02:48:48.772933: Current learning rate: 0.0028 
2025-03-16 02:53:24.033787: train_loss -0.8429 
2025-03-16 02:53:24.034111: val_loss -0.6825 
2025-03-16 02:53:24.034192: Pseudo dice [0.8323] 
2025-03-16 02:53:24.034294: Epoch time: 275.27 s 
2025-03-16 02:53:26.291815:  
2025-03-16 02:53:26.292047: Epoch 758 
2025-03-16 02:53:26.292161: Current learning rate: 0.00279 
2025-03-16 02:58:01.249881: train_loss -0.8546 
2025-03-16 02:58:01.250188: val_loss -0.7346 
2025-03-16 02:58:01.250274: Pseudo dice [0.8413] 
2025-03-16 02:58:01.250374: Epoch time: 274.96 s 
2025-03-16 02:58:03.233069:  
2025-03-16 02:58:03.233274: Epoch 759 
2025-03-16 02:58:03.233406: Current learning rate: 0.00278 
2025-03-16 03:02:38.692842: train_loss -0.8502 
2025-03-16 03:02:38.693187: val_loss -0.7782 
2025-03-16 03:02:38.693297: Pseudo dice [0.8851] 
2025-03-16 03:02:38.693401: Epoch time: 275.46 s 
2025-03-16 03:02:40.660301:  
2025-03-16 03:02:40.660528: Epoch 760 
2025-03-16 03:02:40.660672: Current learning rate: 0.00277 
2025-03-16 03:07:16.242821: train_loss -0.8502 
2025-03-16 03:07:16.243164: val_loss -0.7574 
2025-03-16 03:07:16.243246: Pseudo dice [0.8368] 
2025-03-16 03:07:16.243341: Epoch time: 275.59 s 
2025-03-16 03:07:18.233674:  
2025-03-16 03:07:18.233922: Epoch 761 
2025-03-16 03:07:18.234040: Current learning rate: 0.00276 
2025-03-16 03:11:53.819962: train_loss -0.8442 
2025-03-16 03:11:53.820263: val_loss -0.7467 
2025-03-16 03:11:53.820339: Pseudo dice [0.8243] 
2025-03-16 03:11:53.820425: Epoch time: 275.59 s 
2025-03-16 03:11:55.782254:  
2025-03-16 03:11:55.782466: Epoch 762 
2025-03-16 03:11:55.782583: Current learning rate: 0.00275 
2025-03-16 03:16:30.714684: train_loss -0.8513 
2025-03-16 03:16:30.714987: val_loss -0.7149 
2025-03-16 03:16:30.715066: Pseudo dice [0.8234] 
2025-03-16 03:16:30.715154: Epoch time: 274.94 s 
2025-03-16 03:16:32.716926:  
2025-03-16 03:16:32.717181: Epoch 763 
2025-03-16 03:16:32.717325: Current learning rate: 0.00274 
2025-03-16 03:21:07.638389: train_loss -0.8657 
2025-03-16 03:21:07.638720: val_loss -0.739 
2025-03-16 03:21:07.638817: Pseudo dice [0.8187] 
2025-03-16 03:21:07.638926: Epoch time: 274.93 s 
2025-03-16 03:21:09.622144:  
2025-03-16 03:21:09.622321: Epoch 764 
2025-03-16 03:21:09.622438: Current learning rate: 0.00273 
2025-03-16 03:25:44.750687: train_loss -0.8561 
2025-03-16 03:25:44.751034: val_loss -0.7382 
2025-03-16 03:25:44.751114: Pseudo dice [0.8202] 
2025-03-16 03:25:44.751209: Epoch time: 275.13 s 
2025-03-16 03:25:46.746115:  
2025-03-16 03:25:46.746290: Epoch 765 
2025-03-16 03:25:46.746403: Current learning rate: 0.00272 
2025-03-16 03:30:21.600458: train_loss -0.8689 
2025-03-16 03:30:21.600739: val_loss -0.735 
2025-03-16 03:30:21.600817: Pseudo dice [0.8359] 
2025-03-16 03:30:21.600920: Epoch time: 274.86 s 
2025-03-16 03:30:23.883188:  
2025-03-16 03:30:23.883421: Epoch 766 
2025-03-16 03:30:23.883841: Current learning rate: 0.00271 
2025-03-16 03:34:58.808273: train_loss -0.8633 
2025-03-16 03:34:58.808582: val_loss -0.7092 
2025-03-16 03:34:58.808660: Pseudo dice [0.8435] 
2025-03-16 03:34:58.808743: Epoch time: 274.93 s 
2025-03-16 03:35:00.796873:  
2025-03-16 03:35:00.797096: Epoch 767 
2025-03-16 03:35:00.797211: Current learning rate: 0.0027 
2025-03-16 03:39:35.704960: train_loss -0.8575 
2025-03-16 03:39:35.705253: val_loss -0.771 
2025-03-16 03:39:35.705325: Pseudo dice [0.8542] 
2025-03-16 03:39:35.705409: Epoch time: 274.91 s 
2025-03-16 03:39:37.683418:  
2025-03-16 03:39:37.683716: Epoch 768 
2025-03-16 03:39:37.683859: Current learning rate: 0.00268 
2025-03-16 03:44:12.661174: train_loss -0.8621 
2025-03-16 03:44:12.661455: val_loss -0.7641 
2025-03-16 03:44:12.661530: Pseudo dice [0.8577] 
2025-03-16 03:44:12.661609: Epoch time: 274.98 s 
2025-03-16 03:44:14.655437:  
2025-03-16 03:44:14.655737: Epoch 769 
2025-03-16 03:44:14.655861: Current learning rate: 0.00267 
2025-03-16 03:48:49.215757: train_loss -0.8501 
2025-03-16 03:48:49.216080: val_loss -0.7755 
2025-03-16 03:48:49.216176: Pseudo dice [0.8583] 
2025-03-16 03:48:49.216272: Epoch time: 274.56 s 
2025-03-16 03:48:51.205415:  
2025-03-16 03:48:51.205681: Epoch 770 
2025-03-16 03:48:51.205801: Current learning rate: 0.00266 
2025-03-16 03:53:25.792233: train_loss -0.8617 
2025-03-16 03:53:25.792512: val_loss -0.7578 
2025-03-16 03:53:25.792589: Pseudo dice [0.8145] 
2025-03-16 03:53:25.792676: Epoch time: 274.59 s 
2025-03-16 03:53:27.786554:  
2025-03-16 03:53:27.786849: Epoch 771 
2025-03-16 03:53:27.787012: Current learning rate: 0.00265 
2025-03-16 03:58:02.333701: train_loss -0.8617 
2025-03-16 03:58:02.334061: val_loss -0.7559 
2025-03-16 03:58:02.334151: Pseudo dice [0.8491] 
2025-03-16 03:58:02.334249: Epoch time: 274.55 s 
2025-03-16 03:58:04.350364:  
2025-03-16 03:58:04.350667: Epoch 772 
2025-03-16 03:58:04.350802: Current learning rate: 0.00264 
2025-03-16 04:02:38.917314: train_loss -0.8591 
2025-03-16 04:02:38.917657: val_loss -0.7806 
2025-03-16 04:02:38.917759: Pseudo dice [0.8487] 
2025-03-16 04:02:38.917860: Epoch time: 274.57 s 
2025-03-16 04:02:41.230522:  
2025-03-16 04:02:41.230822: Epoch 773 
2025-03-16 04:02:41.230968: Current learning rate: 0.00263 
2025-03-16 04:07:15.783850: train_loss -0.8532 
2025-03-16 04:07:15.784172: val_loss -0.6883 
2025-03-16 04:07:15.784268: Pseudo dice [0.7951] 
2025-03-16 04:07:15.784361: Epoch time: 274.56 s 
2025-03-16 04:07:17.779269:  
2025-03-16 04:07:17.779548: Epoch 774 
2025-03-16 04:07:17.779673: Current learning rate: 0.00262 
2025-03-16 04:11:52.319953: train_loss -0.852 
2025-03-16 04:11:52.320283: val_loss -0.7263 
2025-03-16 04:11:52.320366: Pseudo dice [0.8216] 
2025-03-16 04:11:52.320462: Epoch time: 274.54 s 
2025-03-16 04:11:54.318680:  
2025-03-16 04:11:54.318868: Epoch 775 
2025-03-16 04:11:54.319007: Current learning rate: 0.00261 
2025-03-16 04:16:28.647101: train_loss -0.8632 
2025-03-16 04:16:28.647396: val_loss -0.706 
2025-03-16 04:16:28.647489: Pseudo dice [0.8469] 
2025-03-16 04:16:28.647572: Epoch time: 274.33 s 
2025-03-16 04:16:30.627641:  
2025-03-16 04:16:30.627834: Epoch 776 
2025-03-16 04:16:30.627954: Current learning rate: 0.0026 
2025-03-16 04:21:04.832186: train_loss -0.8625 
2025-03-16 04:21:04.832516: val_loss -0.696 
2025-03-16 04:21:04.832613: Pseudo dice [0.7949] 
2025-03-16 04:21:04.832715: Epoch time: 274.21 s 
2025-03-16 04:21:06.826511:  
2025-03-16 04:21:06.826695: Epoch 777 
2025-03-16 04:21:06.826811: Current learning rate: 0.00259 
2025-03-16 04:25:41.794312: train_loss -0.8543 
2025-03-16 04:25:41.794622: val_loss -0.6002 
2025-03-16 04:25:41.794698: Pseudo dice [0.73] 
2025-03-16 04:25:41.794780: Epoch time: 274.97 s 
2025-03-16 04:25:43.786735:  
2025-03-16 04:25:43.786926: Epoch 778 
2025-03-16 04:25:43.787042: Current learning rate: 0.00258 
2025-03-16 04:30:18.832361: train_loss -0.8553 
2025-03-16 04:30:18.832680: val_loss -0.7738 
2025-03-16 04:30:18.832770: Pseudo dice [0.8422] 
2025-03-16 04:30:18.832873: Epoch time: 275.05 s 
2025-03-16 04:30:20.825121:  
2025-03-16 04:30:20.825411: Epoch 779 
2025-03-16 04:30:20.825542: Current learning rate: 0.00257 
2025-03-16 04:34:56.001415: train_loss -0.8507 
2025-03-16 04:34:56.001749: val_loss -0.7483 
2025-03-16 04:34:56.001832: Pseudo dice [0.8094] 
2025-03-16 04:34:56.001934: Epoch time: 275.18 s 
2025-03-16 04:34:58.006342:  
2025-03-16 04:34:58.006551: Epoch 780 
2025-03-16 04:34:58.006667: Current learning rate: 0.00256 
2025-03-16 04:39:33.121345: train_loss -0.8434 
2025-03-16 04:39:33.121646: val_loss -0.5855 
2025-03-16 04:39:33.121738: Pseudo dice [0.7629] 
2025-03-16 04:39:33.121820: Epoch time: 275.12 s 
2025-03-16 04:39:35.127321:  
2025-03-16 04:39:35.127668: Epoch 781 
2025-03-16 04:39:35.127819: Current learning rate: 0.00255 
2025-03-16 04:44:10.239593: train_loss -0.8506 
2025-03-16 04:44:10.239983: val_loss -0.6271 
2025-03-16 04:44:10.240075: Pseudo dice [0.8075] 
2025-03-16 04:44:10.240168: Epoch time: 275.12 s 
2025-03-16 04:44:12.236479:  
2025-03-16 04:44:12.236662: Epoch 782 
2025-03-16 04:44:12.236780: Current learning rate: 0.00254 
2025-03-16 04:48:47.245167: train_loss -0.8637 
2025-03-16 04:48:47.245536: val_loss -0.7005 
2025-03-16 04:48:47.245616: Pseudo dice [0.8025] 
2025-03-16 04:48:47.245713: Epoch time: 275.01 s 
2025-03-16 04:48:49.257001:  
2025-03-16 04:48:49.257309: Epoch 783 
2025-03-16 04:48:49.257434: Current learning rate: 0.00253 
2025-03-16 04:53:24.261815: train_loss -0.8617 
2025-03-16 04:53:24.262174: val_loss -0.7276 
2025-03-16 04:53:24.262264: Pseudo dice [0.8262] 
2025-03-16 04:53:24.262362: Epoch time: 275.01 s 
2025-03-16 04:53:26.261298:  
2025-03-16 04:53:26.261467: Epoch 784 
2025-03-16 04:53:26.261601: Current learning rate: 0.00252 
2025-03-16 04:58:01.251039: train_loss -0.8623 
2025-03-16 04:58:01.251343: val_loss -0.7153 
2025-03-16 04:58:01.251425: Pseudo dice [0.8446] 
2025-03-16 04:58:01.251569: Epoch time: 274.99 s 
2025-03-16 04:58:03.253800:  
2025-03-16 04:58:03.253985: Epoch 785 
2025-03-16 04:58:03.254100: Current learning rate: 0.00251 
2025-03-16 05:02:38.737846: train_loss -0.8555 
2025-03-16 05:02:38.738130: val_loss -0.7614 
2025-03-16 05:02:38.738208: Pseudo dice [0.8517] 
2025-03-16 05:02:38.738293: Epoch time: 275.49 s 
2025-03-16 05:02:40.730892:  
2025-03-16 05:02:40.731167: Epoch 786 
2025-03-16 05:02:40.731312: Current learning rate: 0.0025 
2025-03-16 05:07:16.175648: train_loss -0.8548 
2025-03-16 05:07:16.175998: val_loss -0.7461 
2025-03-16 05:07:16.176089: Pseudo dice [0.8589] 
2025-03-16 05:07:16.176189: Epoch time: 275.45 s 
2025-03-16 05:07:18.178457:  
2025-03-16 05:07:18.178698: Epoch 787 
2025-03-16 05:07:18.178831: Current learning rate: 0.00249 
2025-03-16 05:11:52.965732: train_loss -0.864 
2025-03-16 05:11:52.966038: val_loss -0.769 
2025-03-16 05:11:52.966116: Pseudo dice [0.8432] 
2025-03-16 05:11:52.966202: Epoch time: 274.79 s 
2025-03-16 05:11:54.959945:  
2025-03-16 05:11:54.960143: Epoch 788 
2025-03-16 05:11:54.960258: Current learning rate: 0.00248 
2025-03-16 05:16:29.859493: train_loss -0.8593 
2025-03-16 05:16:29.859833: val_loss -0.7103 
2025-03-16 05:16:29.859921: Pseudo dice [0.8366] 
2025-03-16 05:16:29.860020: Epoch time: 274.9 s 
2025-03-16 05:16:32.158734:  
2025-03-16 05:16:32.158931: Epoch 789 
2025-03-16 05:16:32.159082: Current learning rate: 0.00247 
2025-03-16 05:21:07.109328: train_loss -0.8677 
2025-03-16 05:21:07.109672: val_loss -0.7233 
2025-03-16 05:21:07.109805: Pseudo dice [0.8408] 
2025-03-16 05:21:07.109904: Epoch time: 274.95 s 
2025-03-16 05:21:09.099392:  
2025-03-16 05:21:09.099633: Epoch 790 
2025-03-16 05:21:09.099754: Current learning rate: 0.00245 
2025-03-16 05:25:44.067040: train_loss -0.8642 
2025-03-16 05:25:44.067338: val_loss -0.7546 
2025-03-16 05:25:44.067425: Pseudo dice [0.831] 
2025-03-16 05:25:44.067514: Epoch time: 274.97 s 
2025-03-16 05:25:46.044147:  
2025-03-16 05:25:46.044337: Epoch 791 
2025-03-16 05:25:46.044450: Current learning rate: 0.00244 
2025-03-16 05:30:20.555175: train_loss -0.8627 
2025-03-16 05:30:20.555498: val_loss -0.7391 
2025-03-16 05:30:20.555605: Pseudo dice [0.8254] 
2025-03-16 05:30:20.555731: Epoch time: 274.51 s 
2025-03-16 05:30:22.553011:  
2025-03-16 05:30:22.553278: Epoch 792 
2025-03-16 05:30:22.553395: Current learning rate: 0.00243 
2025-03-16 05:34:57.169763: train_loss -0.8636 
2025-03-16 05:34:57.170077: val_loss -0.7543 
2025-03-16 05:34:57.170161: Pseudo dice [0.8456] 
2025-03-16 05:34:57.170263: Epoch time: 274.62 s 
2025-03-16 05:34:59.172457:  
2025-03-16 05:34:59.172653: Epoch 793 
2025-03-16 05:34:59.172775: Current learning rate: 0.00242 
2025-03-16 05:39:33.741575: train_loss -0.8726 
2025-03-16 05:39:33.741928: val_loss -0.7205 
2025-03-16 05:39:33.742012: Pseudo dice [0.8306] 
2025-03-16 05:39:33.742110: Epoch time: 274.57 s 
2025-03-16 05:39:35.731402:  
2025-03-16 05:39:35.731657: Epoch 794 
2025-03-16 05:39:35.731779: Current learning rate: 0.00241 
2025-03-16 05:44:10.278878: train_loss -0.8598 
2025-03-16 05:44:10.279186: val_loss -0.6717 
2025-03-16 05:44:10.279277: Pseudo dice [0.8348] 
2025-03-16 05:44:10.279377: Epoch time: 274.55 s 
2025-03-16 05:44:10.279441: Yayy! New best EMA pseudo Dice: 0.83 
2025-03-16 05:44:13.702554:  
2025-03-16 05:44:13.702756: Epoch 795 
2025-03-16 05:44:13.702875: Current learning rate: 0.0024 
2025-03-16 05:48:48.110862: train_loss -0.8614 
2025-03-16 05:48:48.111299: val_loss -0.7263 
2025-03-16 05:48:48.111394: Pseudo dice [0.8297] 
2025-03-16 05:48:48.111487: Epoch time: 274.41 s 
2025-03-16 05:48:50.089863:  
2025-03-16 05:48:50.089998: Epoch 796 
2025-03-16 05:48:50.090112: Current learning rate: 0.00239 
2025-03-16 05:53:24.474154: train_loss -0.8572 
2025-03-16 05:53:24.474462: val_loss -0.7264 
2025-03-16 05:53:24.474549: Pseudo dice [0.8359] 
2025-03-16 05:53:24.474646: Epoch time: 274.39 s 
2025-03-16 05:53:24.474705: Yayy! New best EMA pseudo Dice: 0.8306 
2025-03-16 05:53:28.156452:  
2025-03-16 05:53:28.156703: Epoch 797 
2025-03-16 05:53:28.156817: Current learning rate: 0.00238 
2025-03-16 05:58:02.662004: train_loss -0.8655 
2025-03-16 05:58:02.662395: val_loss -0.6128 
2025-03-16 05:58:02.662490: Pseudo dice [0.726] 
2025-03-16 05:58:02.662569: Epoch time: 274.51 s 
2025-03-16 05:58:04.672163:  
2025-03-16 05:58:04.672347: Epoch 798 
2025-03-16 05:58:04.672464: Current learning rate: 0.00237 
2025-03-16 06:02:39.331783: train_loss -0.8482 
2025-03-16 06:02:39.332136: val_loss -0.6997 
2025-03-16 06:02:39.332228: Pseudo dice [0.7582] 
2025-03-16 06:02:39.332393: Epoch time: 274.66 s 
2025-03-16 06:02:41.328867:  
2025-03-16 06:02:41.329060: Epoch 799 
2025-03-16 06:02:41.329175: Current learning rate: 0.00236 
2025-03-16 06:07:16.231432: train_loss -0.8558 
2025-03-16 06:07:16.231770: val_loss -0.7615 
2025-03-16 06:07:16.231853: Pseudo dice [0.8647] 
2025-03-16 06:07:16.231950: Epoch time: 274.91 s 
2025-03-16 06:07:19.813281:  
2025-03-16 06:07:19.813463: Epoch 800 
2025-03-16 06:07:19.813577: Current learning rate: 0.00235 
2025-03-16 06:11:54.896734: train_loss -0.8588 
2025-03-16 06:11:54.897059: val_loss -0.7756 
2025-03-16 06:11:54.897151: Pseudo dice [0.8545] 
2025-03-16 06:11:54.897247: Epoch time: 275.09 s 
2025-03-16 06:11:56.902317:  
2025-03-16 06:11:56.902531: Epoch 801 
2025-03-16 06:11:56.902646: Current learning rate: 0.00234 
2025-03-16 06:16:32.082102: train_loss -0.8646 
2025-03-16 06:16:32.082419: val_loss -0.7902 
2025-03-16 06:16:32.082496: Pseudo dice [0.8426] 
2025-03-16 06:16:32.082593: Epoch time: 275.18 s 
2025-03-16 06:16:34.085244:  
2025-03-16 06:16:34.085421: Epoch 802 
2025-03-16 06:16:34.085546: Current learning rate: 0.00233 
2025-03-16 06:21:09.224198: train_loss -0.8637 
2025-03-16 06:21:09.224552: val_loss -0.6963 
2025-03-16 06:21:09.224637: Pseudo dice [0.8293] 
2025-03-16 06:21:09.224784: Epoch time: 275.14 s 
2025-03-16 06:21:11.230179:  
2025-03-16 06:21:11.230362: Epoch 803 
2025-03-16 06:21:11.230476: Current learning rate: 0.00232 
2025-03-16 06:25:46.300056: train_loss -0.8672 
2025-03-16 06:25:46.300358: val_loss -0.7755 
2025-03-16 06:25:46.300479: Pseudo dice [0.8467] 
2025-03-16 06:25:46.300573: Epoch time: 275.07 s 
2025-03-16 06:25:48.596756:  
2025-03-16 06:25:48.596950: Epoch 804 
2025-03-16 06:25:48.597062: Current learning rate: 0.00231 
2025-03-16 06:30:23.694618: train_loss -0.8626 
2025-03-16 06:30:23.694960: val_loss -0.7446 
2025-03-16 06:30:23.695044: Pseudo dice [0.8534] 
2025-03-16 06:30:23.695142: Epoch time: 275.1 s 
2025-03-16 06:30:25.697625:  
2025-03-16 06:30:25.697822: Epoch 805 
2025-03-16 06:30:25.697940: Current learning rate: 0.0023 
2025-03-16 06:35:00.632375: train_loss -0.8446 
2025-03-16 06:35:00.632872: val_loss -0.7394 
2025-03-16 06:35:00.632969: Pseudo dice [0.8063] 
2025-03-16 06:35:00.633051: Epoch time: 274.94 s 
2025-03-16 06:35:02.645785:  
2025-03-16 06:35:02.646003: Epoch 806 
2025-03-16 06:35:02.646135: Current learning rate: 0.00229 
2025-03-16 06:39:37.758792: train_loss -0.8401 
2025-03-16 06:39:37.759096: val_loss -0.536 
2025-03-16 06:39:37.759183: Pseudo dice [0.5366] 
2025-03-16 06:39:37.759326: Epoch time: 275.12 s 
2025-03-16 06:39:39.756904:  
2025-03-16 06:39:39.757143: Epoch 807 
2025-03-16 06:39:39.757288: Current learning rate: 0.00228 
2025-03-16 06:44:14.456124: train_loss -0.8408 
2025-03-16 06:44:14.456438: val_loss -0.7477 
2025-03-16 06:44:14.456623: Pseudo dice [0.842] 
2025-03-16 06:44:14.456720: Epoch time: 274.7 s 
2025-03-16 06:44:16.464972:  
2025-03-16 06:44:16.465180: Epoch 808 
2025-03-16 06:44:16.465302: Current learning rate: 0.00226 
2025-03-16 06:48:50.571619: train_loss -0.861 
2025-03-16 06:48:50.571989: val_loss -0.6974 
2025-03-16 06:48:50.572073: Pseudo dice [0.7421] 
2025-03-16 06:48:50.572170: Epoch time: 274.11 s 
2025-03-16 06:48:52.579041:  
2025-03-16 06:48:52.579241: Epoch 809 
2025-03-16 06:48:52.579386: Current learning rate: 0.00225 
