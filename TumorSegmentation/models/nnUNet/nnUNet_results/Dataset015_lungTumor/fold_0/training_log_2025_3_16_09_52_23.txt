
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 
2025-03-16 09:52:27.095773: Using torch.compile... 
2025-03-16 09:52:32.446346: do_dummy_2d_data_aug: False 
2025-03-16 09:52:32.446963: Using splits from existing split file: /mrhung_nguyen_minh_quang_108/workspace/train/nnUNet_preprocessed/Dataset015_lungTumor/splits_final.json 
2025-03-16 09:52:32.447138: The split file contains 5 splits. 
2025-03-16 09:52:32.447181: Desired fold for training: 0 
2025-03-16 09:52:32.447219: This split has 92 training and 23 validation cases. 

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [80, 192, 160], 'median_image_size_in_voxels': [251.0, 512.0, 512.0], 'spacing': [1.25, 0.78125, 0.78125], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.ResidualEncoderUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_blocks_per_stage': [1, 3, 4, 6, 6, 6], 'n_conv_per_stage_decoder': [1, 1, 1, 1, 1], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset015_lungTumor', 'plans_name': 'nnUNetResEncUNetMPlans', 'original_median_spacing_after_transp': [1.25, 0.78125, 0.78125], 'original_median_shape_after_transp': [251, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'nnUNetPlannerResEncM', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 2750.0, 'mean': -292.26348876953125, 'median': -205.0, 'min': -1270.0, 'percentile_00_5': -1024.0, 'percentile_99_5': 308.0, 'std': 352.5594787597656}}} 
 
2025-03-16 09:52:34.148715: Unable to plot network architecture: nnUNet_compile is enabled! 
2025-03-16 09:52:34.205129:  
2025-03-16 09:52:34.205368: Epoch 800 
2025-03-16 09:52:34.205564: Current learning rate: 0.00235 
2025-03-16 09:57:21.978271: train_loss -0.8644 
2025-03-16 09:57:21.978511: val_loss -0.7934 
2025-03-16 09:57:21.978587: Pseudo dice [0.8793] 
2025-03-16 09:57:21.978698: Epoch time: 287.78 s 
2025-03-16 09:57:24.016924:  
2025-03-16 09:57:24.017138: Epoch 801 
2025-03-16 09:57:24.017263: Current learning rate: 0.00234 
2025-03-16 10:01:58.560781: train_loss -0.86 
2025-03-16 10:01:58.561079: val_loss -0.7301 
2025-03-16 10:01:58.561158: Pseudo dice [0.8391] 
2025-03-16 10:01:58.561240: Epoch time: 274.55 s 
2025-03-16 10:02:00.552836:  
2025-03-16 10:02:00.553157: Epoch 802 
2025-03-16 10:02:00.553284: Current learning rate: 0.00233 
2025-03-16 10:06:35.249434: train_loss -0.8599 
2025-03-16 10:06:35.249751: val_loss -0.7688 
2025-03-16 10:06:35.249827: Pseudo dice [0.8628] 
2025-03-16 10:06:35.249914: Epoch time: 274.7 s 
2025-03-16 10:06:37.227229:  
2025-03-16 10:06:37.227476: Epoch 803 
2025-03-16 10:06:37.227650: Current learning rate: 0.00232 
2025-03-16 10:11:11.515427: train_loss -0.8631 
2025-03-16 10:11:11.515772: val_loss -0.7362 
2025-03-16 10:11:11.515868: Pseudo dice [0.8245] 
2025-03-16 10:11:11.515965: Epoch time: 274.29 s 
2025-03-16 10:11:13.495933:  
2025-03-16 10:11:13.496150: Epoch 804 
2025-03-16 10:11:13.496275: Current learning rate: 0.00231 
2025-03-16 10:15:47.695015: train_loss -0.8628 
2025-03-16 10:15:47.695336: val_loss -0.7205 
2025-03-16 10:15:47.695421: Pseudo dice [0.7869] 
2025-03-16 10:15:47.695529: Epoch time: 274.2 s 
2025-03-16 10:15:49.699491:  
2025-03-16 10:15:49.699719: Epoch 805 
2025-03-16 10:15:49.699843: Current learning rate: 0.0023 
2025-03-16 10:20:23.734981: train_loss -0.8592 
2025-03-16 10:20:23.735372: val_loss -0.7204 
2025-03-16 10:20:23.735473: Pseudo dice [0.8441] 
2025-03-16 10:20:23.735556: Epoch time: 274.04 s 
2025-03-16 10:20:25.734430:  
2025-03-16 10:20:25.734636: Epoch 806 
2025-03-16 10:20:25.734751: Current learning rate: 0.00229 
2025-03-16 10:24:59.953557: train_loss -0.8688 
2025-03-16 10:24:59.953870: val_loss -0.6492 
2025-03-16 10:24:59.953954: Pseudo dice [0.7771] 
2025-03-16 10:24:59.954052: Epoch time: 274.22 s 
2025-03-16 10:25:01.946797:  
2025-03-16 10:25:01.947047: Epoch 807 
2025-03-16 10:25:01.947188: Current learning rate: 0.00228 
2025-03-16 10:29:36.330553: train_loss -0.8514 
2025-03-16 10:29:36.330940: val_loss -0.7388 
2025-03-16 10:29:36.331026: Pseudo dice [0.8623] 
2025-03-16 10:29:36.331124: Epoch time: 274.39 s 
2025-03-16 10:29:38.636106:  
2025-03-16 10:29:38.636384: Epoch 808 
2025-03-16 10:29:38.636518: Current learning rate: 0.00226 
2025-03-16 10:34:12.945709: train_loss -0.8495 
2025-03-16 10:34:12.945997: val_loss -0.6985 
2025-03-16 10:34:12.946079: Pseudo dice [0.8153] 
2025-03-16 10:34:12.946165: Epoch time: 274.31 s 
2025-03-16 10:34:14.932651:  
2025-03-16 10:34:14.932883: Epoch 809 
2025-03-16 10:34:14.933003: Current learning rate: 0.00225 
2025-03-16 10:38:49.445722: train_loss -0.8568 
2025-03-16 10:38:49.446005: val_loss -0.749 
2025-03-16 10:38:49.446079: Pseudo dice [0.825] 
2025-03-16 10:38:49.446163: Epoch time: 274.52 s 
2025-03-16 10:38:51.458852:  
2025-03-16 10:38:51.459114: Epoch 810 
2025-03-16 10:38:51.459230: Current learning rate: 0.00224 
2025-03-16 10:43:25.929644: train_loss -0.8564 
2025-03-16 10:43:25.929951: val_loss -0.6719 
2025-03-16 10:43:25.930028: Pseudo dice [0.7933] 
2025-03-16 10:43:25.930117: Epoch time: 274.47 s 
2025-03-16 10:43:27.961785:  
2025-03-16 10:43:27.962050: Epoch 811 
2025-03-16 10:43:27.962207: Current learning rate: 0.00223 
2025-03-16 10:48:02.775872: train_loss -0.8656 
2025-03-16 10:48:02.776191: val_loss -0.7216 
2025-03-16 10:48:02.776282: Pseudo dice [0.827] 
2025-03-16 10:48:02.776426: Epoch time: 274.82 s 
2025-03-16 10:48:04.812905:  
2025-03-16 10:48:04.813148: Epoch 812 
2025-03-16 10:48:04.813259: Current learning rate: 0.00222 
2025-03-16 10:52:39.954543: train_loss -0.8569 
2025-03-16 10:52:39.954836: val_loss -0.7424 
2025-03-16 10:52:39.954915: Pseudo dice [0.8071] 
2025-03-16 10:52:39.954999: Epoch time: 275.15 s 
2025-03-16 10:52:41.983000:  
2025-03-16 10:52:41.983242: Epoch 813 
2025-03-16 10:52:41.983363: Current learning rate: 0.00221 
2025-03-16 10:57:16.919834: train_loss -0.863 
2025-03-16 10:57:16.920151: val_loss -0.6578 
2025-03-16 10:57:16.920236: Pseudo dice [0.6658] 
2025-03-16 10:57:16.920329: Epoch time: 274.94 s 
2025-03-16 10:57:18.940338:  
2025-03-16 10:57:18.940542: Epoch 814 
2025-03-16 10:57:18.940722: Current learning rate: 0.0022 
2025-03-16 11:01:54.017536: train_loss -0.8563 
2025-03-16 11:01:54.017893: val_loss -0.7574 
2025-03-16 11:01:54.018012: Pseudo dice [0.8006] 
2025-03-16 11:01:54.018096: Epoch time: 275.08 s 
2025-03-16 11:01:56.292896:  
2025-03-16 11:01:56.293127: Epoch 815 
2025-03-16 11:01:56.293280: Current learning rate: 0.00219 
2025-03-16 11:06:31.041801: train_loss -0.8621 
2025-03-16 11:06:31.042104: val_loss -0.6862 
2025-03-16 11:06:31.042182: Pseudo dice [0.8219] 
2025-03-16 11:06:31.042280: Epoch time: 274.75 s 
2025-03-16 11:06:33.058222:  
2025-03-16 11:06:33.058420: Epoch 816 
2025-03-16 11:06:33.058532: Current learning rate: 0.00218 
2025-03-16 11:11:07.514625: train_loss -0.8638 
2025-03-16 11:11:07.514909: val_loss -0.6966 
2025-03-16 11:11:07.514990: Pseudo dice [0.7852] 
2025-03-16 11:11:07.515092: Epoch time: 274.46 s 
2025-03-16 11:11:09.490630:  
2025-03-16 11:11:09.490890: Epoch 817 
2025-03-16 11:11:09.491006: Current learning rate: 0.00217 
2025-03-16 11:15:44.131577: train_loss -0.8519 
2025-03-16 11:15:44.131912: val_loss -0.6697 
2025-03-16 11:15:44.131997: Pseudo dice [0.8204] 
2025-03-16 11:15:44.132095: Epoch time: 274.64 s 
2025-03-16 11:15:46.120614:  
2025-03-16 11:15:46.120841: Epoch 818 
2025-03-16 11:15:46.121023: Current learning rate: 0.00216 
2025-03-16 11:20:20.849398: train_loss -0.8623 
2025-03-16 11:20:20.849744: val_loss -0.7247 
2025-03-16 11:20:20.849826: Pseudo dice [0.8013] 
2025-03-16 11:20:20.849921: Epoch time: 274.73 s 
2025-03-16 11:20:22.881366:  
2025-03-16 11:20:22.881582: Epoch 819 
2025-03-16 11:20:22.881709: Current learning rate: 0.00215 
2025-03-16 11:24:57.534019: train_loss -0.8594 
2025-03-16 11:24:57.534297: val_loss -0.7536 
2025-03-16 11:24:57.534373: Pseudo dice [0.8701] 
2025-03-16 11:24:57.534456: Epoch time: 274.66 s 
2025-03-16 11:24:59.430802:  
2025-03-16 11:24:59.431075: Epoch 820 
2025-03-16 11:24:59.431190: Current learning rate: 0.00214 
2025-03-16 11:29:34.025322: train_loss -0.8461 
2025-03-16 11:29:34.025691: val_loss -0.492 
2025-03-16 11:29:34.025780: Pseudo dice [0.4469] 
2025-03-16 11:29:34.025877: Epoch time: 274.6 s 
2025-03-16 11:29:35.926367:  
2025-03-16 11:29:35.926565: Epoch 821 
2025-03-16 11:29:35.926697: Current learning rate: 0.00213 
2025-03-16 11:34:10.915810: train_loss -0.8535 
2025-03-16 11:34:10.916128: val_loss -0.5601 
2025-03-16 11:34:10.916212: Pseudo dice [0.5916] 
2025-03-16 11:34:10.916356: Epoch time: 274.99 s 
2025-03-16 11:34:12.802553:  
2025-03-16 11:34:12.802817: Epoch 822 
2025-03-16 11:34:12.802976: Current learning rate: 0.00212 
2025-03-16 11:38:47.225879: train_loss -0.8608 
2025-03-16 11:38:47.226209: val_loss -0.6142 
2025-03-16 11:38:47.226301: Pseudo dice [0.597] 
2025-03-16 11:38:47.226399: Epoch time: 274.43 s 
2025-03-16 11:38:49.407845:  
2025-03-16 11:38:49.408098: Epoch 823 
2025-03-16 11:38:49.408298: Current learning rate: 0.0021 
2025-03-16 11:43:23.906378: train_loss -0.8678 
2025-03-16 11:43:23.906749: val_loss -0.5575 
2025-03-16 11:43:23.906891: Pseudo dice [0.561] 
2025-03-16 11:43:23.906977: Epoch time: 274.5 s 
2025-03-16 11:43:25.794774:  
2025-03-16 11:43:25.795043: Epoch 824 
2025-03-16 11:43:25.795162: Current learning rate: 0.00209 
2025-03-16 11:48:00.710852: train_loss -0.8604 
2025-03-16 11:48:00.711184: val_loss -0.6868 
2025-03-16 11:48:00.711267: Pseudo dice [0.729] 
2025-03-16 11:48:00.711356: Epoch time: 274.92 s 
2025-03-16 11:48:02.597124:  
2025-03-16 11:48:02.597406: Epoch 825 
2025-03-16 11:48:02.597539: Current learning rate: 0.00208 
2025-03-16 11:52:37.561565: train_loss -0.8591 
2025-03-16 11:52:37.561893: val_loss -0.7394 
2025-03-16 11:52:37.561975: Pseudo dice [0.8668] 
2025-03-16 11:52:37.562071: Epoch time: 274.97 s 
2025-03-16 11:52:39.454523:  
2025-03-16 11:52:39.454802: Epoch 826 
2025-03-16 11:52:39.454925: Current learning rate: 0.00207 
2025-03-16 11:57:14.627183: train_loss -0.8584 
2025-03-16 11:57:14.627461: val_loss -0.7439 
2025-03-16 11:57:14.627541: Pseudo dice [0.8358] 
2025-03-16 11:57:14.627624: Epoch time: 275.18 s 
2025-03-16 11:57:16.517982:  
2025-03-16 11:57:16.518217: Epoch 827 
2025-03-16 11:57:16.518371: Current learning rate: 0.00206 
2025-03-16 12:01:51.509126: train_loss -0.8572 
2025-03-16 12:01:51.509418: val_loss -0.6808 
2025-03-16 12:01:51.509503: Pseudo dice [0.7999] 
2025-03-16 12:01:51.509598: Epoch time: 275.0 s 
2025-03-16 12:01:53.407517:  
2025-03-16 12:01:53.407796: Epoch 828 
2025-03-16 12:01:53.407913: Current learning rate: 0.00205 
2025-03-16 12:06:28.900186: train_loss -0.8408 
2025-03-16 12:06:28.900551: val_loss -0.6958 
2025-03-16 12:06:28.900636: Pseudo dice [0.831] 
2025-03-16 12:06:28.900736: Epoch time: 275.5 s 
2025-03-16 12:06:30.784354:  
2025-03-16 12:06:30.784596: Epoch 829 
2025-03-16 12:06:30.784739: Current learning rate: 0.00204 
2025-03-16 12:11:05.976317: train_loss -0.845 
2025-03-16 12:11:05.976634: val_loss -0.6827 
2025-03-16 12:11:05.976718: Pseudo dice [0.7587] 
2025-03-16 12:11:05.976818: Epoch time: 275.2 s 
2025-03-16 12:11:07.869278:  
2025-03-16 12:11:07.869677: Epoch 830 
2025-03-16 12:11:07.869840: Current learning rate: 0.00203 
2025-03-16 12:15:43.170521: train_loss -0.8544 
2025-03-16 12:15:43.170823: val_loss -0.7218 
2025-03-16 12:15:43.170959: Pseudo dice [0.8199] 
2025-03-16 12:15:43.171075: Epoch time: 275.31 s 
2025-03-16 12:15:45.070289:  
2025-03-16 12:15:45.070486: Epoch 831 
2025-03-16 12:15:45.070600: Current learning rate: 0.00202 
2025-03-16 12:20:20.385001: train_loss -0.86 
2025-03-16 12:20:20.385296: val_loss -0.7236 
2025-03-16 12:20:20.385379: Pseudo dice [0.8139] 
2025-03-16 12:20:20.385481: Epoch time: 275.32 s 
2025-03-16 12:20:22.551350:  
2025-03-16 12:20:22.551580: Epoch 832 
2025-03-16 12:20:22.551708: Current learning rate: 0.00201 
2025-03-16 12:24:57.618243: train_loss -0.8634 
2025-03-16 12:24:57.618585: val_loss -0.7564 
2025-03-16 12:24:57.618671: Pseudo dice [0.8504] 
2025-03-16 12:24:57.618775: Epoch time: 275.07 s 
2025-03-16 12:24:59.515160:  
2025-03-16 12:24:59.515438: Epoch 833 
2025-03-16 12:24:59.515556: Current learning rate: 0.002 
2025-03-16 12:29:34.452511: train_loss -0.861 
2025-03-16 12:29:34.452853: val_loss -0.7073 
2025-03-16 12:29:34.452935: Pseudo dice [0.7563] 
2025-03-16 12:29:34.453046: Epoch time: 274.94 s 
2025-03-16 12:29:36.347879:  
2025-03-16 12:29:36.348150: Epoch 834 
2025-03-16 12:29:36.348295: Current learning rate: 0.00199 
2025-03-16 12:34:11.325962: train_loss -0.8617 
2025-03-16 12:34:11.326254: val_loss -0.7363 
2025-03-16 12:34:11.326410: Pseudo dice [0.8497] 
2025-03-16 12:34:11.326542: Epoch time: 274.98 s 
2025-03-16 12:34:13.217105:  
2025-03-16 12:34:13.217364: Epoch 835 
2025-03-16 12:34:13.217499: Current learning rate: 0.00198 
2025-03-16 12:38:48.266937: train_loss -0.8655 
2025-03-16 12:38:48.267251: val_loss -0.6992 
2025-03-16 12:38:48.267335: Pseudo dice [0.8435] 
2025-03-16 12:38:48.267444: Epoch time: 275.05 s 
2025-03-16 12:38:50.186824:  
2025-03-16 12:38:50.187031: Epoch 836 
2025-03-16 12:38:50.187144: Current learning rate: 0.00196 
2025-03-16 12:43:25.080386: train_loss -0.8523 
2025-03-16 12:43:25.080801: val_loss -0.5997 
2025-03-16 12:43:25.080887: Pseudo dice [0.6697] 
2025-03-16 12:43:25.080980: Epoch time: 274.9 s 
2025-03-16 12:43:26.976013:  
2025-03-16 12:43:26.976301: Epoch 837 
2025-03-16 12:43:26.976416: Current learning rate: 0.00195 
2025-03-16 12:48:01.892189: train_loss -0.8529 
2025-03-16 12:48:01.892486: val_loss -0.6583 
2025-03-16 12:48:01.892559: Pseudo dice [0.7035] 
2025-03-16 12:48:01.892652: Epoch time: 274.92 s 
2025-03-16 12:48:03.782456:  
2025-03-16 12:48:03.782751: Epoch 838 
2025-03-16 12:48:03.782884: Current learning rate: 0.00194 
2025-03-16 12:52:38.204109: train_loss -0.851 
2025-03-16 12:52:38.204437: val_loss -0.6963 
2025-03-16 12:52:38.204521: Pseudo dice [0.8445] 
2025-03-16 12:52:38.204620: Epoch time: 274.43 s 
2025-03-16 12:52:40.090359:  
2025-03-16 12:52:40.090619: Epoch 839 
2025-03-16 12:52:40.090742: Current learning rate: 0.00193 
2025-03-16 12:57:14.732887: train_loss -0.8659 
2025-03-16 12:57:14.733246: val_loss -0.7573 
2025-03-16 12:57:14.733324: Pseudo dice [0.8567] 
2025-03-16 12:57:14.733414: Epoch time: 274.65 s 
2025-03-16 12:57:17.008287:  
2025-03-16 12:57:17.008501: Epoch 840 
2025-03-16 12:57:17.008622: Current learning rate: 0.00192 
2025-03-16 13:01:51.347131: train_loss -0.8685 
2025-03-16 13:01:51.347425: val_loss -0.72 
2025-03-16 13:01:51.347500: Pseudo dice [0.8463] 
2025-03-16 13:01:51.347770: Epoch time: 274.34 s 
2025-03-16 13:01:53.231237:  
2025-03-16 13:01:53.231516: Epoch 841 
2025-03-16 13:01:53.231642: Current learning rate: 0.00191 
2025-03-16 13:06:27.642386: train_loss -0.8615 
2025-03-16 13:06:27.642697: val_loss -0.7293 
2025-03-16 13:06:27.642780: Pseudo dice [0.7926] 
2025-03-16 13:06:27.642880: Epoch time: 274.42 s 
2025-03-16 13:06:29.579128:  
2025-03-16 13:06:29.579418: Epoch 842 
2025-03-16 13:06:29.579534: Current learning rate: 0.0019 
2025-03-16 13:11:04.055213: train_loss -0.8618 
2025-03-16 13:11:04.055527: val_loss -0.7006 
2025-03-16 13:11:04.055617: Pseudo dice [0.7484] 
2025-03-16 13:11:04.055718: Epoch time: 274.48 s 
2025-03-16 13:11:05.949474:  
2025-03-16 13:11:05.949773: Epoch 843 
2025-03-16 13:11:05.949901: Current learning rate: 0.00189 
2025-03-16 13:15:40.458929: train_loss -0.8652 
2025-03-16 13:15:40.459293: val_loss -0.7509 
2025-03-16 13:15:40.459396: Pseudo dice [0.8147] 
2025-03-16 13:15:40.459491: Epoch time: 274.51 s 
2025-03-16 13:15:42.334210:  
2025-03-16 13:15:42.334492: Epoch 844 
2025-03-16 13:15:42.334604: Current learning rate: 0.00188 
2025-03-16 13:20:17.112689: train_loss -0.8627 
2025-03-16 13:20:17.113015: val_loss -0.7559 
2025-03-16 13:20:17.113098: Pseudo dice [0.8319] 
2025-03-16 13:20:17.113207: Epoch time: 274.78 s 
2025-03-16 13:20:19.016479:  
2025-03-16 13:20:19.016766: Epoch 845 
2025-03-16 13:20:19.016931: Current learning rate: 0.00187 
2025-03-16 13:24:54.002162: train_loss -0.8611 
2025-03-16 13:24:54.002485: val_loss -0.7487 
2025-03-16 13:24:54.002562: Pseudo dice [0.8335] 
2025-03-16 13:24:54.002660: Epoch time: 274.99 s 
2025-03-16 13:24:55.893778:  
2025-03-16 13:24:55.894051: Epoch 846 
2025-03-16 13:24:55.894211: Current learning rate: 0.00186 
2025-03-16 13:29:30.864028: train_loss -0.866 
2025-03-16 13:29:30.864336: val_loss -0.7457 
2025-03-16 13:29:30.864419: Pseudo dice [0.85] 
2025-03-16 13:29:30.864506: Epoch time: 274.97 s 
2025-03-16 13:29:32.781705:  
2025-03-16 13:29:32.781949: Epoch 847 
2025-03-16 13:29:32.782110: Current learning rate: 0.00185 
2025-03-16 13:34:07.929811: train_loss -0.8651 
2025-03-16 13:34:07.930107: val_loss -0.7293 
2025-03-16 13:34:07.930187: Pseudo dice [0.8343] 
2025-03-16 13:34:07.930293: Epoch time: 275.15 s 
2025-03-16 13:34:09.812576:  
2025-03-16 13:34:09.812795: Epoch 848 
2025-03-16 13:34:09.812920: Current learning rate: 0.00184 
2025-03-16 13:38:44.885561: train_loss -0.8638 
2025-03-16 13:38:44.885896: val_loss -0.7575 
2025-03-16 13:38:44.885984: Pseudo dice [0.8273] 
2025-03-16 13:38:44.886069: Epoch time: 275.08 s 
2025-03-16 13:38:47.075425:  
2025-03-16 13:38:47.075677: Epoch 849 
2025-03-16 13:38:47.075845: Current learning rate: 0.00182 
2025-03-16 13:43:22.149841: train_loss -0.8655 
2025-03-16 13:43:22.150176: val_loss -0.7372 
2025-03-16 13:43:22.150263: Pseudo dice [0.8099] 
2025-03-16 13:43:22.150366: Epoch time: 275.08 s 
2025-03-16 13:43:25.431965:  
2025-03-16 13:43:25.432276: Epoch 850 
2025-03-16 13:43:25.432391: Current learning rate: 0.00181 
2025-03-16 13:48:00.345775: train_loss -0.8686 
2025-03-16 13:48:00.346164: val_loss -0.6363 
2025-03-16 13:48:00.346261: Pseudo dice [0.7119] 
2025-03-16 13:48:00.346346: Epoch time: 274.92 s 
2025-03-16 13:48:02.231686:  
2025-03-16 13:48:02.232013: Epoch 851 
2025-03-16 13:48:02.232214: Current learning rate: 0.0018 
2025-03-16 13:52:38.158581: train_loss -0.8649 
2025-03-16 13:52:38.158952: val_loss -0.7479 
2025-03-16 13:52:38.159048: Pseudo dice [0.8449] 
2025-03-16 13:52:38.159143: Epoch time: 275.93 s 
2025-03-16 13:52:40.047786:  
2025-03-16 13:52:40.048034: Epoch 852 
2025-03-16 13:52:40.048193: Current learning rate: 0.00179 
2025-03-16 13:57:15.857496: train_loss -0.8647 
2025-03-16 13:57:15.857818: val_loss -0.7907 
2025-03-16 13:57:15.857904: Pseudo dice [0.8765] 
2025-03-16 13:57:15.858003: Epoch time: 275.81 s 
2025-03-16 13:57:17.730598:  
2025-03-16 13:57:17.730842: Epoch 853 
2025-03-16 13:57:17.730959: Current learning rate: 0.00178 
2025-03-16 14:01:51.060502: train_loss -0.8593 
2025-03-16 14:01:51.060833: val_loss -0.6882 
2025-03-16 14:01:51.060915: Pseudo dice [0.7889] 
2025-03-16 14:01:51.061017: Epoch time: 273.33 s 
2025-03-16 14:01:52.940384:  
2025-03-16 14:01:52.940594: Epoch 854 
2025-03-16 14:01:52.940717: Current learning rate: 0.00177 
2025-03-16 14:06:26.638855: train_loss -0.861 
2025-03-16 14:06:26.639191: val_loss -0.7725 
2025-03-16 14:06:26.639318: Pseudo dice [0.8405] 
2025-03-16 14:06:26.639412: Epoch time: 273.7 s 
2025-03-16 14:06:28.522006:  
2025-03-16 14:06:28.522233: Epoch 855 
2025-03-16 14:06:28.522346: Current learning rate: 0.00176 
2025-03-16 14:11:01.646483: train_loss -0.8591 
2025-03-16 14:11:01.646873: val_loss -0.7729 
2025-03-16 14:11:01.646966: Pseudo dice [0.8512] 
2025-03-16 14:11:01.647055: Epoch time: 273.13 s 
2025-03-16 14:11:03.529723:  
2025-03-16 14:11:03.530026: Epoch 856 
2025-03-16 14:11:03.530205: Current learning rate: 0.00175 
2025-03-16 14:15:36.591130: train_loss -0.8679 
2025-03-16 14:15:36.591517: val_loss -0.7653 
2025-03-16 14:15:36.591598: Pseudo dice [0.8405] 
2025-03-16 14:15:36.591691: Epoch time: 273.07 s 
2025-03-16 14:15:38.730850:  
2025-03-16 14:15:38.731035: Epoch 857 
2025-03-16 14:15:38.731218: Current learning rate: 0.00174 
2025-03-16 14:20:12.207645: train_loss -0.8549 
2025-03-16 14:20:12.207973: val_loss -0.7325 
2025-03-16 14:20:12.208052: Pseudo dice [0.8143] 
2025-03-16 14:20:12.208146: Epoch time: 273.48 s 
2025-03-16 14:20:14.080684:  
2025-03-16 14:20:14.080965: Epoch 858 
2025-03-16 14:20:14.081130: Current learning rate: 0.00173 
2025-03-16 14:24:47.098368: train_loss -0.8755 
2025-03-16 14:24:47.098689: val_loss -0.7799 
2025-03-16 14:24:47.098781: Pseudo dice [0.8831] 
2025-03-16 14:24:47.098878: Epoch time: 273.02 s 
2025-03-16 14:24:48.993044:  
2025-03-16 14:24:48.993342: Epoch 859 
2025-03-16 14:24:48.993474: Current learning rate: 0.00172 
2025-03-16 14:29:21.801292: train_loss -0.8753 
2025-03-16 14:29:21.801598: val_loss -0.7224 
2025-03-16 14:29:21.801707: Pseudo dice [0.833] 
2025-03-16 14:29:21.801838: Epoch time: 272.81 s 
2025-03-16 14:29:23.685366:  
2025-03-16 14:29:23.685622: Epoch 860 
2025-03-16 14:29:23.685748: Current learning rate: 0.0017 
2025-03-16 14:33:56.758750: train_loss -0.8613 
2025-03-16 14:33:56.759066: val_loss -0.7328 
2025-03-16 14:33:56.759143: Pseudo dice [0.8031] 
2025-03-16 14:33:56.759243: Epoch time: 273.08 s 
2025-03-16 14:33:58.651049:  
2025-03-16 14:33:58.651272: Epoch 861 
2025-03-16 14:33:58.651392: Current learning rate: 0.00169 
2025-03-16 14:38:32.447484: train_loss -0.8595 
2025-03-16 14:38:32.447818: val_loss -0.7825 
2025-03-16 14:38:32.447932: Pseudo dice [0.8732] 
2025-03-16 14:38:32.448072: Epoch time: 273.8 s 
2025-03-16 14:38:34.342230:  
2025-03-16 14:38:34.342450: Epoch 862 
2025-03-16 14:38:34.342590: Current learning rate: 0.00168 
2025-03-16 14:43:09.322700: train_loss -0.863 
2025-03-16 14:43:09.322989: val_loss -0.7531 
2025-03-16 14:43:09.323068: Pseudo dice [0.838] 
2025-03-16 14:43:09.323151: Epoch time: 274.98 s 
2025-03-16 14:43:11.193166:  
2025-03-16 14:43:11.193446: Epoch 863 
2025-03-16 14:43:11.193565: Current learning rate: 0.00167 
2025-03-16 14:47:45.865098: train_loss -0.866 
2025-03-16 14:47:45.865495: val_loss -0.6572 
2025-03-16 14:47:45.865605: Pseudo dice [0.7799] 
2025-03-16 14:47:45.865686: Epoch time: 274.68 s 
2025-03-16 14:47:47.751197:  
2025-03-16 14:47:47.751426: Epoch 864 
2025-03-16 14:47:47.751552: Current learning rate: 0.00166 
2025-03-16 14:52:21.978673: train_loss -0.8607 
2025-03-16 14:52:21.978991: val_loss -0.7385 
2025-03-16 14:52:21.979085: Pseudo dice [0.8313] 
2025-03-16 14:52:21.979188: Epoch time: 274.23 s 
2025-03-16 14:52:23.848834:  
2025-03-16 14:52:23.849102: Epoch 865 
2025-03-16 14:52:23.849224: Current learning rate: 0.00165 
2025-03-16 14:56:58.219682: train_loss -0.8657 
2025-03-16 14:56:58.220063: val_loss -0.6477 
2025-03-16 14:56:58.220142: Pseudo dice [0.7577] 
2025-03-16 14:56:58.220238: Epoch time: 274.37 s 
2025-03-16 14:57:00.370690:  
2025-03-16 14:57:00.370886: Epoch 866 
2025-03-16 14:57:00.371096: Current learning rate: 0.00164 
2025-03-16 15:01:34.996689: train_loss -0.864 
2025-03-16 15:01:34.996989: val_loss -0.7069 
2025-03-16 15:01:34.997079: Pseudo dice [0.8441] 
2025-03-16 15:01:34.997176: Epoch time: 274.63 s 
2025-03-16 15:01:36.882938:  
2025-03-16 15:01:36.883212: Epoch 867 
2025-03-16 15:01:36.883346: Current learning rate: 0.00163 
2025-03-16 15:06:11.608632: train_loss -0.858 
2025-03-16 15:06:11.608962: val_loss -0.7245 
2025-03-16 15:06:11.609045: Pseudo dice [0.8395] 
2025-03-16 15:06:11.609157: Epoch time: 274.73 s 
2025-03-16 15:06:13.483166:  
2025-03-16 15:06:13.483388: Epoch 868 
2025-03-16 15:06:13.483530: Current learning rate: 0.00162 
2025-03-16 15:10:48.004748: train_loss -0.8652 
2025-03-16 15:10:48.005043: val_loss -0.7213 
2025-03-16 15:10:48.005124: Pseudo dice [0.8214] 
2025-03-16 15:10:48.005221: Epoch time: 274.53 s 
2025-03-16 15:10:49.889517:  
2025-03-16 15:10:49.889739: Epoch 869 
2025-03-16 15:10:49.889851: Current learning rate: 0.00161 
2025-03-16 15:15:25.023334: train_loss -0.8744 
2025-03-16 15:15:25.023663: val_loss -0.7409 
2025-03-16 15:15:25.023760: Pseudo dice [0.8604] 
2025-03-16 15:15:25.023861: Epoch time: 275.14 s 
2025-03-16 15:15:26.905725:  
2025-03-16 15:15:26.905943: Epoch 870 
2025-03-16 15:15:26.906070: Current learning rate: 0.00159 
2025-03-16 15:20:01.949762: train_loss -0.8689 
2025-03-16 15:20:01.950137: val_loss -0.7235 
2025-03-16 15:20:01.950218: Pseudo dice [0.8657] 
2025-03-16 15:20:01.950311: Epoch time: 275.05 s 
2025-03-16 15:20:04.001762:  
2025-03-16 15:20:04.002017: Epoch 871 
2025-03-16 15:20:04.002166: Current learning rate: 0.00158 
2025-03-16 15:24:38.735377: train_loss -0.8719 
2025-03-16 15:24:38.735709: val_loss -0.7714 
2025-03-16 15:24:38.735810: Pseudo dice [0.8735] 
2025-03-16 15:24:38.735909: Epoch time: 274.74 s 
2025-03-16 15:24:38.735983: Yayy! New best EMA pseudo Dice: 0.8346 
2025-03-16 15:24:42.031044:  
2025-03-16 15:24:42.031230: Epoch 872 
2025-03-16 15:24:42.031381: Current learning rate: 0.00157 
2025-03-16 15:29:16.824479: train_loss -0.8682 
2025-03-16 15:29:16.824802: val_loss -0.7598 
2025-03-16 15:29:16.824887: Pseudo dice [0.8451] 
2025-03-16 15:29:16.824985: Epoch time: 274.8 s 
2025-03-16 15:29:16.825044: Yayy! New best EMA pseudo Dice: 0.8356 
2025-03-16 15:29:20.117637:  
2025-03-16 15:29:20.117898: Epoch 873 
2025-03-16 15:29:20.118015: Current learning rate: 0.00156 
2025-03-16 15:33:54.901937: train_loss -0.8658 
2025-03-16 15:33:54.902278: val_loss -0.7457 
2025-03-16 15:33:54.902389: Pseudo dice [0.8382] 
2025-03-16 15:33:54.902490: Epoch time: 274.79 s 
2025-03-16 15:33:54.902550: Yayy! New best EMA pseudo Dice: 0.8359 
2025-03-16 15:33:58.431688:  
2025-03-16 15:33:58.431932: Epoch 874 
2025-03-16 15:33:58.432071: Current learning rate: 0.00155 
2025-03-16 15:38:33.352725: train_loss -0.8585 
2025-03-16 15:38:33.353068: val_loss -0.7362 
2025-03-16 15:38:33.353155: Pseudo dice [0.8361] 
2025-03-16 15:38:33.353251: Epoch time: 274.92 s 
2025-03-16 15:38:33.353343: Yayy! New best EMA pseudo Dice: 0.8359 
2025-03-16 15:38:36.591292:  
2025-03-16 15:38:36.591501: Epoch 875 
2025-03-16 15:38:36.591680: Current learning rate: 0.00154 
2025-03-16 15:43:11.686817: train_loss -0.8668 
2025-03-16 15:43:11.687124: val_loss -0.7711 
2025-03-16 15:43:11.687233: Pseudo dice [0.8637] 
2025-03-16 15:43:11.687356: Epoch time: 275.1 s 
2025-03-16 15:43:11.687414: Yayy! New best EMA pseudo Dice: 0.8387 
2025-03-16 15:43:15.014518:  
2025-03-16 15:43:15.014742: Epoch 876 
2025-03-16 15:43:15.014892: Current learning rate: 0.00153 
2025-03-16 15:47:49.973703: train_loss -0.8681 
2025-03-16 15:47:49.974004: val_loss -0.6564 
2025-03-16 15:47:49.974081: Pseudo dice [0.8079] 
2025-03-16 15:47:49.974165: Epoch time: 274.96 s 
2025-03-16 15:47:51.856886:  
2025-03-16 15:47:51.857173: Epoch 877 
2025-03-16 15:47:51.857288: Current learning rate: 0.00152 
2025-03-16 15:52:26.757830: train_loss -0.8636 
2025-03-16 15:52:26.758160: val_loss -0.6769 
2025-03-16 15:52:26.758258: Pseudo dice [0.806] 
2025-03-16 15:52:26.758357: Epoch time: 274.9 s 
2025-03-16 15:52:28.644007:  
2025-03-16 15:52:28.644299: Epoch 878 
2025-03-16 15:52:28.644428: Current learning rate: 0.00151 
2025-03-16 15:57:03.673132: train_loss -0.8649 
2025-03-16 15:57:03.673460: val_loss -0.7515 
2025-03-16 15:57:03.673556: Pseudo dice [0.8577] 
2025-03-16 15:57:03.673653: Epoch time: 275.03 s 
2025-03-16 15:57:05.552227:  
2025-03-16 15:57:05.552455: Epoch 879 
2025-03-16 15:57:05.552583: Current learning rate: 0.00149 
2025-03-16 16:01:40.649639: train_loss -0.8694 
2025-03-16 16:01:40.650017: val_loss -0.7263 
2025-03-16 16:01:40.650097: Pseudo dice [0.8467] 
2025-03-16 16:01:40.650203: Epoch time: 275.1 s 
2025-03-16 16:01:42.538515:  
2025-03-16 16:01:42.538753: Epoch 880 
2025-03-16 16:01:42.538879: Current learning rate: 0.00148 
2025-03-16 16:06:17.579250: train_loss -0.865 
2025-03-16 16:06:17.579564: val_loss -0.7087 
2025-03-16 16:06:17.579648: Pseudo dice [0.8298] 
2025-03-16 16:06:17.579746: Epoch time: 275.04 s 
2025-03-16 16:06:19.740977:  
2025-03-16 16:06:19.741198: Epoch 881 
2025-03-16 16:06:19.741312: Current learning rate: 0.00147 
2025-03-16 16:10:54.794743: train_loss -0.8715 
2025-03-16 16:10:54.795020: val_loss -0.7354 
2025-03-16 16:10:54.795092: Pseudo dice [0.8497] 
2025-03-16 16:10:54.795178: Epoch time: 275.06 s 
2025-03-16 16:10:56.659108:  
2025-03-16 16:10:56.659367: Epoch 882 
2025-03-16 16:10:56.659503: Current learning rate: 0.00146 
2025-03-16 16:15:31.898529: train_loss -0.8693 
2025-03-16 16:15:31.898878: val_loss -0.7474 
2025-03-16 16:15:31.898976: Pseudo dice [0.818] 
2025-03-16 16:15:31.899077: Epoch time: 275.24 s 
2025-03-16 16:15:33.776702:  
2025-03-16 16:15:33.776956: Epoch 883 
2025-03-16 16:15:33.777079: Current learning rate: 0.00145 
2025-03-16 16:20:08.730534: train_loss -0.8686 
2025-03-16 16:20:08.730838: val_loss -0.7511 
2025-03-16 16:20:08.730924: Pseudo dice [0.8774] 
2025-03-16 16:20:08.731035: Epoch time: 274.96 s 
2025-03-16 16:20:08.731095: Yayy! New best EMA pseudo Dice: 0.8394 
2025-03-16 16:20:12.013143:  
2025-03-16 16:20:12.013349: Epoch 884 
2025-03-16 16:20:12.013501: Current learning rate: 0.00144 
2025-03-16 16:24:47.185951: train_loss -0.8775 
2025-03-16 16:24:47.186312: val_loss -0.7502 
2025-03-16 16:24:47.186394: Pseudo dice [0.8612] 
2025-03-16 16:24:47.186492: Epoch time: 275.18 s 
2025-03-16 16:24:47.186548: Yayy! New best EMA pseudo Dice: 0.8416 
2025-03-16 16:24:50.518045:  
2025-03-16 16:24:50.518256: Epoch 885 
2025-03-16 16:24:50.518371: Current learning rate: 0.00143 
2025-03-16 16:29:25.540217: train_loss -0.8721 
2025-03-16 16:29:25.540587: val_loss -0.7241 
2025-03-16 16:29:25.540672: Pseudo dice [0.8261] 
2025-03-16 16:29:25.540784: Epoch time: 275.03 s 
2025-03-16 16:29:27.429697:  
2025-03-16 16:29:27.429919: Epoch 886 
2025-03-16 16:29:27.430053: Current learning rate: 0.00142 
2025-03-16 16:34:02.466501: train_loss -0.8749 
2025-03-16 16:34:02.466809: val_loss -0.7747 
2025-03-16 16:34:02.466891: Pseudo dice [0.8663] 
2025-03-16 16:34:02.466992: Epoch time: 275.04 s 
2025-03-16 16:34:02.467051: Yayy! New best EMA pseudo Dice: 0.8426 
2025-03-16 16:34:05.741230:  
2025-03-16 16:34:05.741473: Epoch 887 
2025-03-16 16:34:05.741633: Current learning rate: 0.00141 
2025-03-16 16:38:42.391051: train_loss -0.8646 
2025-03-16 16:38:42.391363: val_loss -0.7589 
2025-03-16 16:38:42.391448: Pseudo dice [0.8633] 
2025-03-16 16:38:42.391551: Epoch time: 276.65 s 
2025-03-16 16:38:42.391608: Yayy! New best EMA pseudo Dice: 0.8447 
2025-03-16 16:38:45.972553:  
2025-03-16 16:38:45.972816: Epoch 888 
2025-03-16 16:38:45.972952: Current learning rate: 0.00139 
2025-03-16 16:43:20.329799: train_loss -0.8712 
2025-03-16 16:43:20.330105: val_loss -0.7361 
2025-03-16 16:43:20.330227: Pseudo dice [0.8518] 
2025-03-16 16:43:20.330328: Epoch time: 274.36 s 
2025-03-16 16:43:20.330386: Yayy! New best EMA pseudo Dice: 0.8454 
2025-03-16 16:43:23.618896:  
2025-03-16 16:43:23.619175: Epoch 889 
2025-03-16 16:43:23.619315: Current learning rate: 0.00138 
2025-03-16 16:47:56.731682: train_loss -0.8748 
2025-03-16 16:47:56.731977: val_loss -0.7299 
2025-03-16 16:47:56.732101: Pseudo dice [0.8596] 
2025-03-16 16:47:56.732218: Epoch time: 273.12 s 
2025-03-16 16:47:56.732280: Yayy! New best EMA pseudo Dice: 0.8468 
2025-03-16 16:48:00.008534:  
2025-03-16 16:48:00.008743: Epoch 890 
2025-03-16 16:48:00.008873: Current learning rate: 0.00137 
2025-03-16 16:52:33.079004: train_loss -0.8636 
2025-03-16 16:52:33.079377: val_loss -0.734 
2025-03-16 16:52:33.079464: Pseudo dice [0.8423] 
2025-03-16 16:52:33.079556: Epoch time: 273.07 s 
2025-03-16 16:52:34.967480:  
2025-03-16 16:52:34.967707: Epoch 891 
2025-03-16 16:52:34.967847: Current learning rate: 0.00136 
2025-03-16 16:57:08.076510: train_loss -0.8662 
2025-03-16 16:57:08.076792: val_loss -0.7537 
2025-03-16 16:57:08.076870: Pseudo dice [0.8596] 
2025-03-16 16:57:08.076961: Epoch time: 273.11 s 
2025-03-16 16:57:08.077076: Yayy! New best EMA pseudo Dice: 0.8477 
2025-03-16 16:57:11.384067:  
2025-03-16 16:57:11.384305: Epoch 892 
2025-03-16 16:57:11.384421: Current learning rate: 0.00135 
2025-03-16 17:01:44.363974: train_loss -0.87 
2025-03-16 17:01:44.364290: val_loss -0.7161 
2025-03-16 17:01:44.364367: Pseudo dice [0.8293] 
2025-03-16 17:01:44.364461: Epoch time: 272.98 s 
2025-03-16 17:01:46.237428:  
2025-03-16 17:01:46.237670: Epoch 893 
2025-03-16 17:01:46.237823: Current learning rate: 0.00134 
2025-03-16 17:06:19.329261: train_loss -0.8687 
2025-03-16 17:06:19.329569: val_loss -0.7448 
2025-03-16 17:06:19.329650: Pseudo dice [0.8366] 
2025-03-16 17:06:19.329752: Epoch time: 273.1 s 
2025-03-16 17:06:21.215866:  
2025-03-16 17:06:21.216134: Epoch 894 
2025-03-16 17:06:21.216310: Current learning rate: 0.00133 
2025-03-16 17:10:54.168205: train_loss -0.8728 
2025-03-16 17:10:54.168590: val_loss -0.7135 
2025-03-16 17:10:54.168764: Pseudo dice [0.7996] 
2025-03-16 17:10:54.168858: Epoch time: 272.96 s 
2025-03-16 17:10:56.043628:  
2025-03-16 17:10:56.043854: Epoch 895 
2025-03-16 17:10:56.043972: Current learning rate: 0.00132 
2025-03-16 17:15:30.138651: train_loss -0.879 
2025-03-16 17:15:30.139034: val_loss -0.7539 
2025-03-16 17:15:30.139115: Pseudo dice [0.8397] 
2025-03-16 17:15:30.139205: Epoch time: 274.1 s 
2025-03-16 17:15:32.289550:  
2025-03-16 17:15:32.289771: Epoch 896 
2025-03-16 17:15:32.289888: Current learning rate: 0.0013 
2025-03-16 17:20:06.512513: train_loss -0.8722 
2025-03-16 17:20:06.512877: val_loss -0.7221 
2025-03-16 17:20:06.512969: Pseudo dice [0.8258] 
2025-03-16 17:20:06.513064: Epoch time: 274.23 s 
2025-03-16 17:20:08.376882:  
2025-03-16 17:20:08.377153: Epoch 897 
2025-03-16 17:20:08.377315: Current learning rate: 0.00129 
2025-03-16 17:24:42.474158: train_loss -0.8675 
2025-03-16 17:24:42.474514: val_loss -0.7103 
2025-03-16 17:24:42.474602: Pseudo dice [0.8419] 
2025-03-16 17:24:42.474697: Epoch time: 274.1 s 
2025-03-16 17:24:44.369353:  
2025-03-16 17:24:44.369604: Epoch 898 
2025-03-16 17:24:44.369781: Current learning rate: 0.00128 
2025-03-16 17:29:18.672210: train_loss -0.8731 
2025-03-16 17:29:18.672530: val_loss -0.737 
2025-03-16 17:29:18.672608: Pseudo dice [0.8479] 
2025-03-16 17:29:18.672709: Epoch time: 274.31 s 
2025-03-16 17:29:20.550768:  
2025-03-16 17:29:20.551049: Epoch 899 
2025-03-16 17:29:20.551168: Current learning rate: 0.00127 
2025-03-16 17:33:54.968525: train_loss -0.8758 
2025-03-16 17:33:54.968807: val_loss -0.7348 
2025-03-16 17:33:54.968893: Pseudo dice [0.8658] 
2025-03-16 17:33:54.968983: Epoch time: 274.42 s 
2025-03-16 17:33:58.259394:  
2025-03-16 17:33:58.259689: Epoch 900 
2025-03-16 17:33:58.259860: Current learning rate: 0.00126 
2025-03-16 17:38:32.139073: train_loss -0.8639 
2025-03-16 17:38:32.139360: val_loss -0.6765 
2025-03-16 17:38:32.139444: Pseudo dice [0.8326] 
2025-03-16 17:38:32.139541: Epoch time: 273.88 s 
2025-03-16 17:38:34.018251:  
2025-03-16 17:38:34.018511: Epoch 901 
2025-03-16 17:38:34.018668: Current learning rate: 0.00125 
2025-03-16 17:43:07.975886: train_loss -0.8726 
2025-03-16 17:43:07.976183: val_loss -0.7185 
2025-03-16 17:43:07.976275: Pseudo dice [0.8439] 
2025-03-16 17:43:07.976382: Epoch time: 273.96 s 
2025-03-16 17:43:09.854593:  
2025-03-16 17:43:09.854855: Epoch 902 
2025-03-16 17:43:09.855004: Current learning rate: 0.00124 
2025-03-16 17:47:43.767648: train_loss -0.8762 
2025-03-16 17:47:43.768006: val_loss -0.6974 
2025-03-16 17:47:43.768131: Pseudo dice [0.8002] 
2025-03-16 17:47:43.768231: Epoch time: 273.92 s 
2025-03-16 17:47:45.655140:  
2025-03-16 17:47:45.655496: Epoch 903 
2025-03-16 17:47:45.655652: Current learning rate: 0.00122 
2025-03-16 17:52:19.442640: train_loss -0.8727 
2025-03-16 17:52:19.442934: val_loss -0.6513 
2025-03-16 17:52:19.443010: Pseudo dice [0.8229] 
2025-03-16 17:52:19.443111: Epoch time: 273.79 s 
2025-03-16 17:52:21.322992:  
2025-03-16 17:52:21.323233: Epoch 904 
2025-03-16 17:52:21.323361: Current learning rate: 0.00121 
2025-03-16 17:56:55.155428: train_loss -0.8733 
2025-03-16 17:56:55.155645: val_loss -0.7513 
2025-03-16 17:56:55.155811: Pseudo dice [0.8577] 
2025-03-16 17:56:55.155908: Epoch time: 273.84 s 
2025-03-16 17:56:57.014568:  
2025-03-16 17:56:57.014797: Epoch 905 
2025-03-16 17:56:57.014956: Current learning rate: 0.0012 
2025-03-16 18:01:31.037915: train_loss -0.8651 
2025-03-16 18:01:31.038277: val_loss -0.6884 
2025-03-16 18:01:31.038362: Pseudo dice [0.8448] 
2025-03-16 18:01:31.038469: Epoch time: 274.03 s 
2025-03-16 18:01:32.902734:  
2025-03-16 18:01:32.902976: Epoch 906 
2025-03-16 18:01:32.903133: Current learning rate: 0.00119 
2025-03-16 18:06:06.994398: train_loss -0.8742 
2025-03-16 18:06:06.994688: val_loss -0.6243 
2025-03-16 18:06:06.994768: Pseudo dice [0.7299] 
2025-03-16 18:06:06.994858: Epoch time: 274.1 s 
2025-03-16 18:06:08.868338:  
2025-03-16 18:06:08.868647: Epoch 907 
2025-03-16 18:06:08.868837: Current learning rate: 0.00118 
2025-03-16 18:10:42.975343: train_loss -0.8724 
2025-03-16 18:10:42.975645: val_loss -0.7359 
2025-03-16 18:10:42.975726: Pseudo dice [0.8419] 
2025-03-16 18:10:42.975840: Epoch time: 274.11 s 
2025-03-16 18:10:44.869902:  
2025-03-16 18:10:44.870116: Epoch 908 
2025-03-16 18:10:44.870288: Current learning rate: 0.00117 
2025-03-16 18:15:19.351036: train_loss -0.8753 
2025-03-16 18:15:19.351341: val_loss -0.7651 
2025-03-16 18:15:19.351418: Pseudo dice [0.8306] 
2025-03-16 18:15:19.351513: Epoch time: 274.49 s 
2025-03-16 18:15:21.253587:  
2025-03-16 18:15:21.253881: Epoch 909 
2025-03-16 18:15:21.254002: Current learning rate: 0.00116 
2025-03-16 18:19:55.843011: train_loss -0.8748 
2025-03-16 18:19:55.843303: val_loss -0.7757 
2025-03-16 18:19:55.843379: Pseudo dice [0.8415] 
2025-03-16 18:19:55.843463: Epoch time: 274.59 s 
2025-03-16 18:19:57.748198:  
2025-03-16 18:19:57.748424: Epoch 910 
2025-03-16 18:19:57.748583: Current learning rate: 0.00115 
2025-03-16 18:24:32.544030: train_loss -0.877 
2025-03-16 18:24:32.544319: val_loss -0.77 
2025-03-16 18:24:32.544397: Pseudo dice [0.8655] 
2025-03-16 18:24:32.544484: Epoch time: 274.8 s 
2025-03-16 18:24:34.416799:  
2025-03-16 18:24:34.417026: Epoch 911 
2025-03-16 18:24:34.417177: Current learning rate: 0.00113 
2025-03-16 18:29:09.077852: train_loss -0.871 
2025-03-16 18:29:09.078145: val_loss -0.7071 
2025-03-16 18:29:09.078224: Pseudo dice [0.8323] 
2025-03-16 18:29:09.078306: Epoch time: 274.66 s 
2025-03-16 18:29:10.959775:  
2025-03-16 18:29:10.960072: Epoch 912 
2025-03-16 18:29:10.960186: Current learning rate: 0.00112 
2025-03-16 18:33:45.709464: train_loss -0.8773 
2025-03-16 18:33:45.709771: val_loss -0.6809 
2025-03-16 18:33:45.709851: Pseudo dice [0.7165] 
2025-03-16 18:33:45.709939: Epoch time: 274.75 s 
2025-03-16 18:33:47.881764:  
2025-03-16 18:33:47.881972: Epoch 913 
2025-03-16 18:33:47.882093: Current learning rate: 0.00111 
2025-03-16 18:38:22.555604: train_loss -0.8778 
2025-03-16 18:38:22.556005: val_loss -0.7304 
2025-03-16 18:38:22.556125: Pseudo dice [0.8295] 
2025-03-16 18:38:22.556204: Epoch time: 274.68 s 
2025-03-16 18:38:24.414817:  
2025-03-16 18:38:24.415055: Epoch 914 
2025-03-16 18:38:24.415173: Current learning rate: 0.0011 
2025-03-16 18:42:59.232779: train_loss -0.8729 
2025-03-16 18:42:59.233093: val_loss -0.7301 
2025-03-16 18:42:59.233173: Pseudo dice [0.8536] 
2025-03-16 18:42:59.233262: Epoch time: 274.82 s 
2025-03-16 18:43:01.105072:  
2025-03-16 18:43:01.105392: Epoch 915 
2025-03-16 18:43:01.105507: Current learning rate: 0.00109 
2025-03-16 18:47:36.148339: train_loss -0.8729 
2025-03-16 18:47:36.148693: val_loss -0.7643 
2025-03-16 18:47:36.148793: Pseudo dice [0.8614] 
2025-03-16 18:47:36.148891: Epoch time: 275.05 s 
2025-03-16 18:47:38.025498:  
2025-03-16 18:47:38.025755: Epoch 916 
2025-03-16 18:47:38.025904: Current learning rate: 0.00108 
2025-03-16 18:52:13.133994: train_loss -0.8759 
2025-03-16 18:52:13.134346: val_loss -0.7314 
2025-03-16 18:52:13.134447: Pseudo dice [0.8456] 
2025-03-16 18:52:13.134542: Epoch time: 275.11 s 
2025-03-16 18:52:15.025580:  
2025-03-16 18:52:15.025877: Epoch 917 
2025-03-16 18:52:15.026027: Current learning rate: 0.00106 
2025-03-16 18:56:50.400907: train_loss -0.8761 
2025-03-16 18:56:50.401303: val_loss -0.7684 
2025-03-16 18:56:50.401379: Pseudo dice [0.8448] 
2025-03-16 18:56:50.401472: Epoch time: 275.38 s 
2025-03-16 18:56:52.282810:  
2025-03-16 18:56:52.283072: Epoch 918 
2025-03-16 18:56:52.283221: Current learning rate: 0.00105 
2025-03-16 19:01:27.358186: train_loss -0.8787 
2025-03-16 19:01:27.358479: val_loss -0.7732 
2025-03-16 19:01:27.358648: Pseudo dice [0.8453] 
2025-03-16 19:01:27.358742: Epoch time: 275.08 s 
2025-03-16 19:01:29.232300:  
2025-03-16 19:01:29.232563: Epoch 919 
2025-03-16 19:01:29.232754: Current learning rate: 0.00104 
2025-03-16 19:06:04.526175: train_loss -0.8719 
2025-03-16 19:06:04.526477: val_loss -0.7852 
2025-03-16 19:06:04.526597: Pseudo dice [0.869] 
2025-03-16 19:06:04.526700: Epoch time: 275.3 s 
2025-03-16 19:06:06.404225:  
2025-03-16 19:06:06.404438: Epoch 920 
2025-03-16 19:06:06.404551: Current learning rate: 0.00103 
2025-03-16 19:10:41.749563: train_loss -0.878 
2025-03-16 19:10:41.749857: val_loss -0.6786 
2025-03-16 19:10:41.749941: Pseudo dice [0.8022] 
2025-03-16 19:10:41.750031: Epoch time: 275.35 s 
2025-03-16 19:10:43.637536:  
2025-03-16 19:10:43.637756: Epoch 921 
2025-03-16 19:10:43.637901: Current learning rate: 0.00102 
2025-03-16 19:15:19.027232: train_loss -0.8763 
2025-03-16 19:15:19.027524: val_loss -0.7453 
2025-03-16 19:15:19.027704: Pseudo dice [0.8487] 
2025-03-16 19:15:19.027811: Epoch time: 275.39 s 
2025-03-16 19:15:21.198514:  
2025-03-16 19:15:21.198712: Epoch 922 
2025-03-16 19:15:21.198828: Current learning rate: 0.00101 
2025-03-16 19:19:56.549475: train_loss -0.8735 
2025-03-16 19:19:56.549826: val_loss -0.7353 
2025-03-16 19:19:56.549912: Pseudo dice [0.8364] 
2025-03-16 19:19:56.550009: Epoch time: 275.35 s 
2025-03-16 19:19:58.429961:  
2025-03-16 19:19:58.430232: Epoch 923 
2025-03-16 19:19:58.430367: Current learning rate: 0.001 
2025-03-16 19:24:33.235296: train_loss -0.8749 
2025-03-16 19:24:33.235599: val_loss -0.7196 
2025-03-16 19:24:33.235729: Pseudo dice [0.8205] 
2025-03-16 19:24:33.235867: Epoch time: 274.81 s 
2025-03-16 19:24:35.115832:  
2025-03-16 19:24:35.116080: Epoch 924 
2025-03-16 19:24:35.116270: Current learning rate: 0.00098 
2025-03-16 19:29:10.027916: train_loss -0.8794 
2025-03-16 19:29:10.028270: val_loss -0.71 
2025-03-16 19:29:10.028348: Pseudo dice [0.829] 
2025-03-16 19:29:10.028443: Epoch time: 274.92 s 
2025-03-16 19:29:11.913688:  
2025-03-16 19:29:11.913926: Epoch 925 
2025-03-16 19:29:11.914085: Current learning rate: 0.00097 
2025-03-16 19:33:46.803145: train_loss -0.8748 
2025-03-16 19:33:46.803479: val_loss -0.724 
2025-03-16 19:33:46.803562: Pseudo dice [0.8579] 
2025-03-16 19:33:46.803687: Epoch time: 274.89 s 
2025-03-16 19:33:48.714851:  
2025-03-16 19:33:48.715110: Epoch 926 
2025-03-16 19:33:48.715249: Current learning rate: 0.00096 
2025-03-16 19:38:23.550315: train_loss -0.879 
2025-03-16 19:38:23.550725: val_loss -0.7182 
2025-03-16 19:38:23.550830: Pseudo dice [0.7933] 
2025-03-16 19:38:23.550911: Epoch time: 274.84 s 
2025-03-16 19:38:25.433186:  
2025-03-16 19:38:25.433408: Epoch 927 
2025-03-16 19:38:25.433540: Current learning rate: 0.00095 
2025-03-16 19:43:00.573465: train_loss -0.8675 
2025-03-16 19:43:00.573761: val_loss -0.7669 
2025-03-16 19:43:00.573842: Pseudo dice [0.8521] 
2025-03-16 19:43:00.574033: Epoch time: 275.14 s 
2025-03-16 19:43:02.447325:  
2025-03-16 19:43:02.447542: Epoch 928 
2025-03-16 19:43:02.447655: Current learning rate: 0.00094 
2025-03-16 19:47:37.610805: train_loss -0.8789 
2025-03-16 19:47:37.611138: val_loss -0.7574 
2025-03-16 19:47:37.611224: Pseudo dice [0.8633] 
2025-03-16 19:47:37.611321: Epoch time: 275.17 s 
2025-03-16 19:47:39.517725:  
2025-03-16 19:47:39.518028: Epoch 929 
2025-03-16 19:47:39.518144: Current learning rate: 0.00092 
2025-03-16 19:52:14.449875: train_loss -0.8799 
2025-03-16 19:52:14.450159: val_loss -0.7263 
2025-03-16 19:52:14.450236: Pseudo dice [0.8066] 
2025-03-16 19:52:14.450322: Epoch time: 274.94 s 
2025-03-16 19:52:16.461085:  
2025-03-16 19:52:16.461271: Epoch 930 
2025-03-16 19:52:16.461411: Current learning rate: 0.00091 
2025-03-16 19:56:51.596681: train_loss -0.881 
2025-03-16 19:56:51.597019: val_loss -0.7591 
2025-03-16 19:56:51.597096: Pseudo dice [0.855] 
2025-03-16 19:56:51.597182: Epoch time: 275.14 s 
2025-03-16 19:56:53.763288:  
2025-03-16 19:56:53.763535: Epoch 931 
2025-03-16 19:56:53.763714: Current learning rate: 0.0009 
2025-03-16 20:01:29.307810: train_loss -0.8794 
2025-03-16 20:01:29.308156: val_loss -0.7151 
2025-03-16 20:01:29.308238: Pseudo dice [0.8372] 
2025-03-16 20:01:29.308335: Epoch time: 275.55 s 
2025-03-16 20:01:31.176968:  
2025-03-16 20:01:31.177238: Epoch 932 
2025-03-16 20:01:31.177374: Current learning rate: 0.00089 
2025-03-16 20:06:06.517292: train_loss -0.876 
2025-03-16 20:06:06.517648: val_loss -0.7645 
2025-03-16 20:06:06.517737: Pseudo dice [0.8418] 
2025-03-16 20:06:06.517838: Epoch time: 275.34 s 
2025-03-16 20:06:08.411027:  
2025-03-16 20:06:08.411285: Epoch 933 
2025-03-16 20:06:08.411398: Current learning rate: 0.00088 
2025-03-16 20:10:43.365561: train_loss -0.8751 
2025-03-16 20:10:43.365924: val_loss -0.7318 
2025-03-16 20:10:43.366005: Pseudo dice [0.8645] 
2025-03-16 20:10:43.366102: Epoch time: 274.96 s 
2025-03-16 20:10:45.242815:  
2025-03-16 20:10:45.243033: Epoch 934 
2025-03-16 20:10:45.243162: Current learning rate: 0.00087 
2025-03-16 20:15:20.204769: train_loss -0.8755 
2025-03-16 20:15:20.205094: val_loss -0.7406 
2025-03-16 20:15:20.205171: Pseudo dice [0.829] 
2025-03-16 20:15:20.205267: Epoch time: 274.97 s 
2025-03-16 20:15:22.077540:  
2025-03-16 20:15:22.077796: Epoch 935 
2025-03-16 20:15:22.078014: Current learning rate: 0.00085 
2025-03-16 20:19:57.007684: train_loss -0.8811 
2025-03-16 20:19:57.008039: val_loss -0.7526 
2025-03-16 20:19:57.008151: Pseudo dice [0.874] 
2025-03-16 20:19:57.008248: Epoch time: 274.93 s 
2025-03-16 20:19:58.914698:  
2025-03-16 20:19:58.914958: Epoch 936 
2025-03-16 20:19:58.915119: Current learning rate: 0.00084 
2025-03-16 20:24:33.412125: train_loss -0.8773 
2025-03-16 20:24:33.412437: val_loss -0.7499 
2025-03-16 20:24:33.412518: Pseudo dice [0.8392] 
2025-03-16 20:24:33.412614: Epoch time: 274.5 s 
2025-03-16 20:24:35.352114:  
2025-03-16 20:24:35.352376: Epoch 937 
2025-03-16 20:24:35.352492: Current learning rate: 0.00083 
2025-03-16 20:29:09.752083: train_loss -0.8727 
2025-03-16 20:29:09.752366: val_loss -0.764 
2025-03-16 20:29:09.752442: Pseudo dice [0.8652] 
2025-03-16 20:29:09.752564: Epoch time: 274.4 s 
2025-03-16 20:29:11.643435:  
2025-03-16 20:29:11.643706: Epoch 938 
2025-03-16 20:29:11.643842: Current learning rate: 0.00082 
2025-03-16 20:33:45.982698: train_loss -0.8818 
2025-03-16 20:33:45.982988: val_loss -0.7864 
2025-03-16 20:33:45.983069: Pseudo dice [0.8641] 
2025-03-16 20:33:45.983158: Epoch time: 274.34 s 
2025-03-16 20:33:47.932437:  
2025-03-16 20:33:47.932662: Epoch 939 
2025-03-16 20:33:47.932873: Current learning rate: 0.00081 
2025-03-16 20:38:22.347952: train_loss -0.8783 
2025-03-16 20:38:22.348295: val_loss -0.7674 
2025-03-16 20:38:22.348401: Pseudo dice [0.8433] 
2025-03-16 20:38:22.348496: Epoch time: 274.42 s 
2025-03-16 20:38:24.580865:  
2025-03-16 20:38:24.581121: Epoch 940 
2025-03-16 20:38:24.581290: Current learning rate: 0.00079 
2025-03-16 20:42:59.104434: train_loss -0.8716 
2025-03-16 20:42:59.104721: val_loss -0.7688 
2025-03-16 20:42:59.104799: Pseudo dice [0.8396] 
2025-03-16 20:42:59.104882: Epoch time: 274.53 s 
2025-03-16 20:43:01.009248:  
2025-03-16 20:43:01.009537: Epoch 941 
2025-03-16 20:43:01.009679: Current learning rate: 0.00078 
2025-03-16 20:47:35.523200: train_loss -0.8792 
2025-03-16 20:47:35.523499: val_loss -0.7191 
2025-03-16 20:47:35.523594: Pseudo dice [0.816] 
2025-03-16 20:47:35.523679: Epoch time: 274.52 s 
2025-03-16 20:47:37.435282:  
2025-03-16 20:47:37.435551: Epoch 942 
2025-03-16 20:47:37.435768: Current learning rate: 0.00077 
2025-03-16 20:52:11.925535: train_loss -0.8682 
2025-03-16 20:52:11.925851: val_loss -0.7601 
2025-03-16 20:52:11.925928: Pseudo dice [0.8648] 
2025-03-16 20:52:11.926011: Epoch time: 274.49 s 
2025-03-16 20:52:13.821756:  
2025-03-16 20:52:13.822047: Epoch 943 
2025-03-16 20:52:13.822228: Current learning rate: 0.00076 
2025-03-16 20:56:48.301923: train_loss -0.8808 
2025-03-16 20:56:48.302234: val_loss -0.7316 
2025-03-16 20:56:48.302320: Pseudo dice [0.8339] 
2025-03-16 20:56:48.302412: Epoch time: 274.48 s 
2025-03-16 20:56:50.183212:  
2025-03-16 20:56:50.183523: Epoch 944 
2025-03-16 20:56:50.183675: Current learning rate: 0.00075 
2025-03-16 21:01:24.915258: train_loss -0.8781 
2025-03-16 21:01:24.915651: val_loss -0.6907 
2025-03-16 21:01:24.915755: Pseudo dice [0.835] 
2025-03-16 21:01:24.915840: Epoch time: 274.74 s 
2025-03-16 21:01:26.799309:  
2025-03-16 21:01:26.799582: Epoch 945 
2025-03-16 21:01:26.799700: Current learning rate: 0.00074 
2025-03-16 21:06:01.666919: train_loss -0.8826 
2025-03-16 21:06:01.667211: val_loss -0.7078 
2025-03-16 21:06:01.667291: Pseudo dice [0.7983] 
2025-03-16 21:06:01.667378: Epoch time: 274.87 s 
2025-03-16 21:06:03.546734:  
2025-03-16 21:06:03.546971: Epoch 946 
2025-03-16 21:06:03.547087: Current learning rate: 0.00072 
2025-03-16 21:10:38.564088: train_loss -0.8765 
2025-03-16 21:10:38.564408: val_loss -0.7692 
2025-03-16 21:10:38.564550: Pseudo dice [0.8492] 
2025-03-16 21:10:38.564649: Epoch time: 275.02 s 
2025-03-16 21:10:40.441372:  
2025-03-16 21:10:40.441627: Epoch 947 
2025-03-16 21:10:40.441782: Current learning rate: 0.00071 
2025-03-16 21:15:15.492603: train_loss -0.8789 
2025-03-16 21:15:15.492942: val_loss -0.7532 
2025-03-16 21:15:15.493035: Pseudo dice [0.8573] 
2025-03-16 21:15:15.493129: Epoch time: 275.06 s 
2025-03-16 21:15:17.375400:  
2025-03-16 21:15:17.375643: Epoch 948 
2025-03-16 21:15:17.375780: Current learning rate: 0.0007 
2025-03-16 21:19:52.498114: train_loss -0.8807 
2025-03-16 21:19:52.498436: val_loss -0.742 
2025-03-16 21:19:52.498533: Pseudo dice [0.8377] 
2025-03-16 21:19:52.498630: Epoch time: 275.13 s 
2025-03-16 21:19:54.657462:  
2025-03-16 21:19:54.657735: Epoch 949 
2025-03-16 21:19:54.657876: Current learning rate: 0.00069 
2025-03-16 21:24:29.696599: train_loss -0.8769 
2025-03-16 21:24:29.696910: val_loss -0.6756 
2025-03-16 21:24:29.697001: Pseudo dice [0.8459] 
2025-03-16 21:24:29.697113: Epoch time: 275.04 s 
2025-03-16 21:24:32.965884:  
2025-03-16 21:24:32.966138: Epoch 950 
2025-03-16 21:24:32.966250: Current learning rate: 0.00067 
2025-03-16 21:29:08.064903: train_loss -0.8717 
2025-03-16 21:29:08.065223: val_loss -0.7286 
2025-03-16 21:29:08.065311: Pseudo dice [0.8521] 
2025-03-16 21:29:08.065411: Epoch time: 275.1 s 
2025-03-16 21:29:09.967953:  
2025-03-16 21:29:09.968157: Epoch 951 
2025-03-16 21:29:09.968270: Current learning rate: 0.00066 
2025-03-16 21:33:45.012959: train_loss -0.8745 
2025-03-16 21:33:45.013250: val_loss -0.6799 
2025-03-16 21:33:45.013369: Pseudo dice [0.8253] 
2025-03-16 21:33:45.013486: Epoch time: 275.05 s 
2025-03-16 21:33:46.887918:  
2025-03-16 21:33:46.888145: Epoch 952 
2025-03-16 21:33:46.888274: Current learning rate: 0.00065 
2025-03-16 21:38:22.274626: train_loss -0.8797 
2025-03-16 21:38:22.274940: val_loss -0.7384 
2025-03-16 21:38:22.275028: Pseudo dice [0.8325] 
2025-03-16 21:38:22.275121: Epoch time: 275.39 s 
2025-03-16 21:38:24.155013:  
2025-03-16 21:38:24.155298: Epoch 953 
2025-03-16 21:38:24.155453: Current learning rate: 0.00064 
2025-03-16 21:42:59.655915: train_loss -0.8779 
2025-03-16 21:42:59.656317: val_loss -0.7217 
2025-03-16 21:42:59.656416: Pseudo dice [0.843] 
2025-03-16 21:42:59.656498: Epoch time: 275.51 s 
2025-03-16 21:43:01.556973:  
2025-03-16 21:43:01.557249: Epoch 954 
2025-03-16 21:43:01.557372: Current learning rate: 0.00063 
2025-03-16 21:47:36.934872: train_loss -0.8825 
2025-03-16 21:47:36.935237: val_loss -0.712 
2025-03-16 21:47:36.935318: Pseudo dice [0.8188] 
2025-03-16 21:47:36.935416: Epoch time: 275.38 s 
2025-03-16 21:47:38.838006:  
2025-03-16 21:47:38.838232: Epoch 955 
2025-03-16 21:47:38.838455: Current learning rate: 0.00061 
2025-03-16 21:52:14.262251: train_loss -0.8818 
2025-03-16 21:52:14.262567: val_loss -0.7162 
2025-03-16 21:52:14.262646: Pseudo dice [0.8482] 
2025-03-16 21:52:14.262734: Epoch time: 275.43 s 
2025-03-16 21:52:16.162017:  
2025-03-16 21:52:16.162271: Epoch 956 
2025-03-16 21:52:16.162422: Current learning rate: 0.0006 
2025-03-16 21:56:51.200505: train_loss -0.882 
2025-03-16 21:56:51.200789: val_loss -0.7635 
2025-03-16 21:56:51.200875: Pseudo dice [0.8388] 
2025-03-16 21:56:51.200975: Epoch time: 275.04 s 
2025-03-16 21:56:53.375664:  
2025-03-16 21:56:53.375897: Epoch 957 
2025-03-16 21:56:53.376009: Current learning rate: 0.00059 
2025-03-16 22:01:28.330039: train_loss -0.8792 
2025-03-16 22:01:28.330317: val_loss -0.6737 
2025-03-16 22:01:28.330392: Pseudo dice [0.8144] 
2025-03-16 22:01:28.330474: Epoch time: 274.96 s 
2025-03-16 22:01:30.221354:  
2025-03-16 22:01:30.221611: Epoch 958 
2025-03-16 22:01:30.221742: Current learning rate: 0.00058 
2025-03-16 22:06:05.188657: train_loss -0.8803 
2025-03-16 22:06:05.188936: val_loss -0.7027 
2025-03-16 22:06:05.189011: Pseudo dice [0.8485] 
2025-03-16 22:06:05.189097: Epoch time: 274.97 s 
2025-03-16 22:06:07.082631:  
2025-03-16 22:06:07.082898: Epoch 959 
2025-03-16 22:06:07.083034: Current learning rate: 0.00056 
2025-03-16 22:10:42.097021: train_loss -0.8839 
2025-03-16 22:10:42.097347: val_loss -0.7554 
2025-03-16 22:10:42.097430: Pseudo dice [0.8439] 
2025-03-16 22:10:42.097583: Epoch time: 275.02 s 
2025-03-16 22:10:43.996857:  
2025-03-16 22:10:43.997114: Epoch 960 
2025-03-16 22:10:43.997248: Current learning rate: 0.00055 
2025-03-16 22:15:18.957464: train_loss -0.8788 
2025-03-16 22:15:18.957782: val_loss -0.7385 
2025-03-16 22:15:18.958004: Pseudo dice [0.8523] 
2025-03-16 22:15:18.958109: Epoch time: 274.96 s 
2025-03-16 22:15:20.850010:  
2025-03-16 22:15:20.850227: Epoch 961 
2025-03-16 22:15:20.850368: Current learning rate: 0.00054 
2025-03-16 22:19:55.809427: train_loss -0.8619 
2025-03-16 22:19:55.809729: val_loss -0.6818 
2025-03-16 22:19:55.809810: Pseudo dice [0.8182] 
2025-03-16 22:19:55.809896: Epoch time: 274.96 s 
2025-03-16 22:19:57.699970:  
2025-03-16 22:19:57.700182: Epoch 962 
2025-03-16 22:19:57.700312: Current learning rate: 0.00053 
2025-03-16 22:24:32.857327: train_loss -0.8791 
2025-03-16 22:24:32.857620: val_loss -0.7497 
2025-03-16 22:24:32.857700: Pseudo dice [0.8419] 
2025-03-16 22:24:32.857794: Epoch time: 275.16 s 
2025-03-16 22:24:34.769045:  
2025-03-16 22:24:34.769273: Epoch 963 
2025-03-16 22:24:34.769438: Current learning rate: 0.00051 
2025-03-16 22:29:09.809573: train_loss -0.887 
2025-03-16 22:29:09.809921: val_loss -0.7563 
2025-03-16 22:29:09.810004: Pseudo dice [0.8476] 
2025-03-16 22:29:09.810099: Epoch time: 275.04 s 
2025-03-16 22:29:11.743708:  
2025-03-16 22:29:11.744038: Epoch 964 
2025-03-16 22:29:11.744211: Current learning rate: 0.0005 
2025-03-16 22:33:47.162318: train_loss -0.8763 
2025-03-16 22:33:47.162608: val_loss -0.716 
2025-03-16 22:33:47.162685: Pseudo dice [0.8616] 
2025-03-16 22:33:47.162766: Epoch time: 275.42 s 
2025-03-16 22:33:49.074938:  
2025-03-16 22:33:49.075177: Epoch 965 
2025-03-16 22:33:49.075316: Current learning rate: 0.00049 
2025-03-16 22:38:24.622042: train_loss -0.8841 
2025-03-16 22:38:24.622380: val_loss -0.7579 
2025-03-16 22:38:24.622463: Pseudo dice [0.8569] 
2025-03-16 22:38:24.622556: Epoch time: 275.55 s 
2025-03-16 22:38:26.827672:  
2025-03-16 22:38:26.827972: Epoch 966 
2025-03-16 22:38:26.828088: Current learning rate: 0.00048 
2025-03-16 22:43:02.251319: train_loss -0.8811 
2025-03-16 22:43:02.251631: val_loss -0.727 
2025-03-16 22:43:02.251761: Pseudo dice [0.8468] 
2025-03-16 22:43:02.251862: Epoch time: 275.43 s 
2025-03-16 22:43:04.145592:  
2025-03-16 22:43:04.145835: Epoch 967 
2025-03-16 22:43:04.145956: Current learning rate: 0.00046 
2025-03-16 22:47:39.677294: train_loss -0.881 
2025-03-16 22:47:39.677591: val_loss -0.7629 
2025-03-16 22:47:39.677672: Pseudo dice [0.8577] 
2025-03-16 22:47:39.677755: Epoch time: 275.54 s 
2025-03-16 22:47:41.602772:  
2025-03-16 22:47:41.603077: Epoch 968 
2025-03-16 22:47:41.603221: Current learning rate: 0.00045 
2025-03-16 22:52:18.306766: train_loss -0.8802 
2025-03-16 22:52:18.307083: val_loss -0.76 
2025-03-16 22:52:18.307167: Pseudo dice [0.8411] 
2025-03-16 22:52:18.307267: Epoch time: 276.71 s 
2025-03-16 22:52:20.221068:  
2025-03-16 22:52:20.221343: Epoch 969 
2025-03-16 22:52:20.221460: Current learning rate: 0.00044 
2025-03-16 22:56:56.194288: train_loss -0.883 
2025-03-16 22:56:56.194607: val_loss -0.748 
2025-03-16 22:56:56.194691: Pseudo dice [0.8757] 
2025-03-16 22:56:56.194789: Epoch time: 275.98 s 
2025-03-16 22:56:58.099648:  
2025-03-16 22:56:58.099927: Epoch 970 
2025-03-16 22:56:58.100048: Current learning rate: 0.00043 
2025-03-16 23:01:32.529651: train_loss -0.8809 
2025-03-16 23:01:32.530030: val_loss -0.7476 
2025-03-16 23:01:32.530116: Pseudo dice [0.8693] 
2025-03-16 23:01:32.530207: Epoch time: 274.43 s 
2025-03-16 23:01:32.530266: Yayy! New best EMA pseudo Dice: 0.8496 
2025-03-16 23:01:35.879703:  
2025-03-16 23:01:35.879930: Epoch 971 
2025-03-16 23:01:35.880050: Current learning rate: 0.00041 
2025-03-16 23:06:10.136963: train_loss -0.889 
2025-03-16 23:06:10.137266: val_loss -0.7416 
2025-03-16 23:06:10.137413: Pseudo dice [0.8613] 
2025-03-16 23:06:10.137516: Epoch time: 274.26 s 
2025-03-16 23:06:10.137578: Yayy! New best EMA pseudo Dice: 0.8508 
2025-03-16 23:06:13.491627:  
2025-03-16 23:06:13.491864: Epoch 972 
2025-03-16 23:06:13.491979: Current learning rate: 0.0004 
2025-03-16 23:10:48.113764: train_loss -0.8812 
2025-03-16 23:10:48.114082: val_loss -0.7309 
2025-03-16 23:10:48.114160: Pseudo dice [0.8473] 
2025-03-16 23:10:48.114244: Epoch time: 274.63 s 
2025-03-16 23:10:50.293317:  
2025-03-16 23:10:50.293587: Epoch 973 
2025-03-16 23:10:50.293701: Current learning rate: 0.00039 
2025-03-16 23:15:24.886792: train_loss -0.8802 
2025-03-16 23:15:24.887087: val_loss -0.7306 
2025-03-16 23:15:24.887176: Pseudo dice [0.8531] 
2025-03-16 23:15:24.887279: Epoch time: 274.6 s 
2025-03-16 23:15:26.796975:  
2025-03-16 23:15:26.797235: Epoch 974 
2025-03-16 23:15:26.797390: Current learning rate: 0.00037 
2025-03-16 23:20:01.244998: train_loss -0.8821 
2025-03-16 23:20:01.245314: val_loss -0.7079 
2025-03-16 23:20:01.245392: Pseudo dice [0.8391] 
2025-03-16 23:20:01.245476: Epoch time: 274.45 s 
2025-03-16 23:20:03.141469:  
2025-03-16 23:20:03.141681: Epoch 975 
2025-03-16 23:20:03.141812: Current learning rate: 0.00036 
2025-03-16 23:24:37.501558: train_loss -0.8861 
2025-03-16 23:24:37.501875: val_loss -0.7394 
2025-03-16 23:24:37.501950: Pseudo dice [0.8535] 
2025-03-16 23:24:37.502091: Epoch time: 274.36 s 
2025-03-16 23:24:39.446672:  
2025-03-16 23:24:39.446971: Epoch 976 
2025-03-16 23:24:39.447104: Current learning rate: 0.00035 
2025-03-16 23:29:13.710305: train_loss -0.8836 
2025-03-16 23:29:13.710726: val_loss -0.6837 
2025-03-16 23:29:13.710831: Pseudo dice [0.8251] 
2025-03-16 23:29:13.710911: Epoch time: 274.27 s 
2025-03-16 23:29:15.635101:  
2025-03-16 23:29:15.635309: Epoch 977 
2025-03-16 23:29:15.635433: Current learning rate: 0.00034 
2025-03-16 23:33:49.976367: train_loss -0.876 
2025-03-16 23:33:49.976660: val_loss -0.7317 
2025-03-16 23:33:49.976743: Pseudo dice [0.8676] 
2025-03-16 23:33:49.976831: Epoch time: 274.35 s 
2025-03-16 23:33:51.885087:  
2025-03-16 23:33:51.885289: Epoch 978 
2025-03-16 23:33:51.885406: Current learning rate: 0.00032 
2025-03-16 23:38:27.194676: train_loss -0.872 
2025-03-16 23:38:27.194991: val_loss -0.7232 
2025-03-16 23:38:27.195069: Pseudo dice [0.857] 
2025-03-16 23:38:27.195152: Epoch time: 275.31 s 
2025-03-16 23:38:29.115470:  
2025-03-16 23:38:29.115697: Epoch 979 
2025-03-16 23:38:29.115864: Current learning rate: 0.00031 
2025-03-16 23:43:03.689335: train_loss -0.8878 
2025-03-16 23:43:03.689610: val_loss -0.6353 
2025-03-16 23:43:03.689683: Pseudo dice [0.8012] 
2025-03-16 23:43:03.689769: Epoch time: 274.58 s 
2025-03-16 23:43:05.602404:  
2025-03-16 23:43:05.602656: Epoch 980 
2025-03-16 23:43:05.602793: Current learning rate: 0.0003 
2025-03-16 23:47:39.873272: train_loss -0.8884 
2025-03-16 23:47:39.873598: val_loss -0.6841 
2025-03-16 23:47:39.873689: Pseudo dice [0.8511] 
2025-03-16 23:47:39.873778: Epoch time: 274.27 s 
2025-03-16 23:47:41.792570:  
2025-03-16 23:47:41.792781: Epoch 981 
2025-03-16 23:47:41.792955: Current learning rate: 0.00028 
2025-03-16 23:52:15.567528: train_loss -0.8862 
2025-03-16 23:52:15.567839: val_loss -0.7573 
2025-03-16 23:52:15.567920: Pseudo dice [0.8529] 
2025-03-16 23:52:15.568018: Epoch time: 273.78 s 
2025-03-16 23:52:17.798889:  
2025-03-16 23:52:17.799150: Epoch 982 
2025-03-16 23:52:17.799274: Current learning rate: 0.00027 
2025-03-16 23:56:51.582419: train_loss -0.8862 
2025-03-16 23:56:51.582817: val_loss -0.7647 
2025-03-16 23:56:51.582890: Pseudo dice [0.8629] 
2025-03-16 23:56:51.582994: Epoch time: 273.79 s 
2025-03-16 23:56:53.477692:  
2025-03-16 23:56:53.477910: Epoch 983 
2025-03-16 23:56:53.478039: Current learning rate: 0.00026 
2025-03-17 00:01:27.466797: train_loss -0.8829 
2025-03-17 00:01:27.467176: val_loss -0.7471 
2025-03-17 00:01:27.467252: Pseudo dice [0.8425] 
2025-03-17 00:01:27.467347: Epoch time: 273.99 s 
2025-03-17 00:01:29.374503:  
2025-03-17 00:01:29.374752: Epoch 984 
2025-03-17 00:01:29.374903: Current learning rate: 0.00024 
2025-03-17 00:06:03.179448: train_loss -0.8875 
2025-03-17 00:06:03.179761: val_loss -0.6751 
2025-03-17 00:06:03.179844: Pseudo dice [0.8038] 
2025-03-17 00:06:03.179948: Epoch time: 273.81 s 
2025-03-17 00:06:05.106310:  
2025-03-17 00:06:05.106552: Epoch 985 
2025-03-17 00:06:05.106677: Current learning rate: 0.00023 
2025-03-17 00:10:39.068814: train_loss -0.8821 
2025-03-17 00:10:39.069141: val_loss -0.7047 
2025-03-17 00:10:39.069223: Pseudo dice [0.8332] 
2025-03-17 00:10:39.069319: Epoch time: 273.97 s 
2025-03-17 00:10:40.985096:  
2025-03-17 00:10:40.985377: Epoch 986 
2025-03-17 00:10:40.985493: Current learning rate: 0.00021 
2025-03-17 00:15:15.473586: train_loss -0.8845 
2025-03-17 00:15:15.473897: val_loss -0.7536 
2025-03-17 00:15:15.473981: Pseudo dice [0.8583] 
2025-03-17 00:15:15.474068: Epoch time: 274.49 s 
2025-03-17 00:15:17.391577:  
2025-03-17 00:15:17.391850: Epoch 987 
2025-03-17 00:15:17.391993: Current learning rate: 0.0002 
2025-03-17 00:19:51.751404: train_loss -0.8847 
2025-03-17 00:19:51.751765: val_loss -0.7152 
2025-03-17 00:19:51.751879: Pseudo dice [0.8372] 
2025-03-17 00:19:51.751991: Epoch time: 274.36 s 
2025-03-17 00:19:53.664502:  
2025-03-17 00:19:53.664762: Epoch 988 
2025-03-17 00:19:53.664933: Current learning rate: 0.00019 
2025-03-17 00:24:27.633961: train_loss -0.8839 
2025-03-17 00:24:27.634315: val_loss -0.6831 
2025-03-17 00:24:27.634411: Pseudo dice [0.8249] 
2025-03-17 00:24:27.634504: Epoch time: 273.97 s 
2025-03-17 00:24:29.549607:  
2025-03-17 00:24:29.549843: Epoch 989 
2025-03-17 00:24:29.550042: Current learning rate: 0.00017 
2025-03-17 00:29:04.223104: train_loss -0.8862 
2025-03-17 00:29:04.223544: val_loss -0.6718 
2025-03-17 00:29:04.223670: Pseudo dice [0.857] 
2025-03-17 00:29:04.223779: Epoch time: 274.68 s 
2025-03-17 00:29:06.443633:  
2025-03-17 00:29:06.443895: Epoch 990 
2025-03-17 00:29:06.444042: Current learning rate: 0.00016 
2025-03-17 00:33:40.211642: train_loss -0.8822 
2025-03-17 00:33:40.211975: val_loss -0.7584 
2025-03-17 00:33:40.212055: Pseudo dice [0.8514] 
2025-03-17 00:33:40.212157: Epoch time: 273.77 s 
2025-03-17 00:33:42.113630:  
2025-03-17 00:33:42.113908: Epoch 991 
2025-03-17 00:33:42.114029: Current learning rate: 0.00014 
2025-03-17 00:38:15.501489: train_loss -0.8772 
2025-03-17 00:38:15.501803: val_loss -0.7949 
2025-03-17 00:38:15.501887: Pseudo dice [0.8657] 
2025-03-17 00:38:15.501972: Epoch time: 273.39 s 
2025-03-17 00:38:17.395575:  
2025-03-17 00:38:17.395875: Epoch 992 
2025-03-17 00:38:17.396015: Current learning rate: 0.00013 
2025-03-17 00:42:50.939187: train_loss -0.8707 
2025-03-17 00:42:50.939480: val_loss -0.7081 
2025-03-17 00:42:50.939618: Pseudo dice [0.8309] 
2025-03-17 00:42:50.939718: Epoch time: 273.55 s 
2025-03-17 00:42:52.854743:  
2025-03-17 00:42:52.854985: Epoch 993 
2025-03-17 00:42:52.855110: Current learning rate: 0.00011 
2025-03-17 00:47:26.446591: train_loss -0.8821 
2025-03-17 00:47:26.446867: val_loss -0.7315 
2025-03-17 00:47:26.446953: Pseudo dice [0.8543] 
2025-03-17 00:47:26.447040: Epoch time: 273.6 s 
2025-03-17 00:47:28.358164:  
2025-03-17 00:47:28.358486: Epoch 994 
2025-03-17 00:47:28.358646: Current learning rate: 0.0001 
2025-03-17 00:52:01.957783: train_loss -0.8815 
2025-03-17 00:52:01.958091: val_loss -0.7702 
2025-03-17 00:52:01.958241: Pseudo dice [0.8395] 
2025-03-17 00:52:01.958334: Epoch time: 273.6 s 
2025-03-17 00:52:03.855656:  
2025-03-17 00:52:03.855879: Epoch 995 
2025-03-17 00:52:03.855993: Current learning rate: 8e-05 
2025-03-17 00:56:37.292670: train_loss -0.8755 
2025-03-17 00:56:37.292953: val_loss -0.7526 
2025-03-17 00:56:37.293029: Pseudo dice [0.8596] 
2025-03-17 00:56:37.293114: Epoch time: 273.44 s 
2025-03-17 00:56:39.202430:  
2025-03-17 00:56:39.202696: Epoch 996 
2025-03-17 00:56:39.202813: Current learning rate: 7e-05 
2025-03-17 01:01:13.191342: train_loss -0.8845 
2025-03-17 01:01:13.191643: val_loss -0.6977 
2025-03-17 01:01:13.191727: Pseudo dice [0.8433] 
2025-03-17 01:01:13.191839: Epoch time: 273.99 s 
2025-03-17 01:01:15.104714:  
2025-03-17 01:01:15.104928: Epoch 997 
2025-03-17 01:01:15.105087: Current learning rate: 5e-05 
2025-03-17 01:05:49.228923: train_loss -0.8895 
2025-03-17 01:05:49.229252: val_loss -0.7322 
2025-03-17 01:05:49.229329: Pseudo dice [0.8612] 
2025-03-17 01:05:49.229422: Epoch time: 274.13 s 
2025-03-17 01:05:51.135395:  
2025-03-17 01:05:51.135607: Epoch 998 
2025-03-17 01:05:51.135721: Current learning rate: 4e-05 
2025-03-17 01:10:25.263551: train_loss -0.8891 
2025-03-17 01:10:25.263906: val_loss -0.7159 
2025-03-17 01:10:25.263991: Pseudo dice [0.824] 
2025-03-17 01:10:25.264087: Epoch time: 274.13 s 
2025-03-17 01:10:27.484247:  
2025-03-17 01:10:27.484508: Epoch 999 
2025-03-17 01:10:27.484640: Current learning rate: 2e-05 
2025-03-17 01:15:01.759358: train_loss -0.8772 
2025-03-17 01:15:01.759696: val_loss -0.6922 
2025-03-17 01:15:01.759839: Pseudo dice [0.838] 
2025-03-17 01:15:01.759933: Epoch time: 274.28 s 
2025-03-17 01:15:04.854066: Training done. 
2025-03-17 01:15:04.920966: Using splits from existing split file: /mrhung_nguyen_minh_quang_108/workspace/train/nnUNet_preprocessed/Dataset015_lungTumor/splits_final.json 
2025-03-17 01:15:04.921237: The split file contains 5 splits. 
2025-03-17 01:15:04.921283: Desired fold for training: 0 
2025-03-17 01:15:04.921319: This split has 92 training and 23 validation cases. 
2025-03-17 01:15:04.921504: predicting lung_006 
2025-03-17 01:15:06.094206: lung_006, shape torch.Size([1, 284, 640, 640]), rank 0 
2025-03-17 01:21:22.742617: predicting lung_015 
2025-03-17 01:21:23.250505: lung_015, shape torch.Size([1, 294, 486, 486]), rank 0 
2025-03-17 01:25:49.689090: predicting lung_022 
2025-03-17 01:25:50.120730: lung_022, shape torch.Size([1, 188, 563, 563]), rank 0 
2025-03-17 01:28:47.837054: predicting lung_027 
2025-03-17 01:28:48.440410: lung_027, shape torch.Size([1, 227, 512, 512]), rank 0 
2025-03-17 01:31:58.971248: predicting lung_034 
2025-03-17 01:31:59.479754: lung_034, shape torch.Size([1, 260, 538, 538]), rank 0 
2025-03-17 01:35:48.076606: predicting lung_051 
2025-03-17 01:35:48.525052: lung_051, shape torch.Size([1, 224, 548, 548]), rank 0 
2025-03-17 01:38:59.219712: predicting lung_054 
2025-03-17 01:38:59.904112: lung_054, shape torch.Size([1, 252, 614, 614]), rank 0 
2025-03-17 01:44:20.130083: predicting lung_065 
2025-03-17 01:44:20.487624: lung_065, shape torch.Size([1, 248, 461, 461]), rank 0 
2025-03-17 01:46:53.010100: predicting lung_066 
2025-03-17 01:46:53.303265: lung_066, shape torch.Size([1, 256, 476, 476]), rank 0 
2025-03-17 01:49:25.708079: predicting lung_068 
2025-03-17 01:49:26.341491: lung_068, shape torch.Size([1, 300, 527, 527]), rank 0 
2025-03-17 01:53:53.114829: predicting lung_073 
2025-03-17 01:53:53.701159: lung_073, shape torch.Size([1, 216, 614, 614]), rank 0 
2025-03-17 01:58:20.601423: predicting lung_078 
2025-03-17 01:58:20.916793: lung_078, shape torch.Size([1, 217, 435, 435]), rank 0 
2025-03-17 02:00:27.968265: predicting lung_083 
2025-03-17 02:00:28.516969: lung_083, shape torch.Size([1, 268, 512, 512]), rank 0 
2025-03-17 02:04:16.852024: predicting lung_086 
2025-03-17 02:04:17.504230: lung_086, shape torch.Size([1, 241, 512, 512]), rank 0 
2025-03-17 02:08:05.996209: predicting lung_089 
2025-03-17 02:08:06.370389: lung_089, shape torch.Size([1, 236, 448, 448]), rank 0 
2025-03-17 02:10:13.515378: predicting lung_090 
2025-03-17 02:10:13.885410: lung_090, shape torch.Size([1, 218, 489, 489]), rank 0 
2025-03-17 02:13:24.594644: predicting lung_109 
2025-03-17 02:13:25.300790: lung_109, shape torch.Size([1, 280, 512, 512]), rank 0 
2025-03-17 02:17:13.948878: predicting lung_111 
2025-03-17 02:17:14.574253: lung_111, shape torch.Size([1, 233, 602, 602]), rank 0 
